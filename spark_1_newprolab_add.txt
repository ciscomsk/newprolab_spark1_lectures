ADD
__ZOOKEEPER
https://zookeeper.apache.org/


__HADOOP
https://hadoop.apache.org/docs/stable/index.html

+https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html

FileSystem - getFileBlockLocations
https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html#getFileBlockLocations-org.apache.hadoop.fs.FileStatus-long-long-

FileSystem - create FSDataOutputStream
https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html#create-org.apache.hadoop.fs.Path-boolean-

FSDataInputStream
https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FSDataInputStream.html

FileSystem
https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java

Configuring Mountable HDFS (FUSE - Filesystem in Userspace)
https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_ig_hdfs_mountable.html


__COMPUTER_SCIENCE
The Google File System
https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf

MapReduce: Simplified Data Processing on Large Clusters
+https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf


__MAPREDUCE+
MapReduce WordCount example
https://github.com/naver/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java


__HIVE
https://cwiki.apache.org/confluence/display/Hive/Home
https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Overview


__DATA_FORMATS
https://parquet.apache.org/docs/
https://pypi.org/project/parquet-tools/
https://mvnrepository.com/artifact/org.apache.parquet/parquet-tools
https://rangareddy.github.io/ParquetTools/


Как класть Parquet - Фёдор Лаврентьев, Moscow Spark #5
https://www.youtube.com/watch?v=VHsvr10b63c&t=512s

+https://cwiki.apache.org/confluence/display/hive/languagemanual+orc

https://avro.apache.org/docs/


__IDEA
Idea -> Settings -> Build, Execution, Deployment -> Build Tools -> sbt
sbt shell -> use for:
1. project reload
2. builds

Implicit hints
https://www.jetbrains.com/help/idea/scala-features-overview-scala.html#scala_hints


__SCALA_LEARNING
+https://www.scala-lang.org/download/

+https://docs.scala-lang.org/api/all.html
+https://www.scala-lang.org/api/2.13.14

https://docs.scala-lang.org/tour/tour-of-scala.html

+https://www.scala-exercises.org/

https://www.scala-lang.org/api/2.13.0/scala-compiler/scala/tools/reflect/ToolBox.html?search=ToolBox

https://docs.scala-lang.org/overviews/quasiquotes/intro.html

+https://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala

+https://stackoverflow.com/questions/32199989/what-are-the-differences-between-final-class-and-sealed-class-in-scala

+https://www.baeldung.com/scala/for-comprehension

+https://docs.scala-lang.org/overviews/collections-2.13/performance-characteristics.html

https://www.lihaoyi.com/post/BenchmarkingScalaCollections.html
https://github.com/mslinn/scala-bench

+https://www.scala-lang.org/files/archive/spec/2.13/05-classes-and-objects.html

+http://www.jesperdj.com/2016/01/08/scala-access-modifiers-and-qualifiers-in-detail/

+https://alvinalexander.com/
http://scalacookbook.com/


__SCALA_TOOLS
https://www.scala-sbt.org/
sbt_shell: clean/compile/run

Для изучения результатов генерации scala-кода можно использовать:
1. scalac -Xprint:all <file>.scala - покажет все шаги работы компилятора
2. javap <file>.class - покажет дизассемблированный код результата


__SCALA_LIBRARIES
https://github.com/pureconfig/pureconfig

+https://github.com/lightbend/config

https://json4s.org/index.html
https://github.com/json4s/json4s

+https://circe.github.io/circe/
https://github.com/circe/circe

https://www.scalatest.org/

http://typelevel.org/frameless/

https://github.com/twitter/scalding


__SCALA_ECOSYSTEM
+https://www.jetbrains.com/idea/download/?section=linux
https://docs.scala-lang.org/getting-started/index.html#open-hello-world-project

+http://ammonite.io/

+https://scastie.scala-lang.org/
https://scalafiddle.io/ - не работает


__JAVA_ECOSYSTEM
https://janino-compiler.github.io/janino/

jvm -D
https://stackoverflow.com/questions/44745261/why-do-jvm-arguments-start-with-d
https://stackoverflow.com/questions/59366184/what-is-d-in-vm-arguments-what-it-indicates-why-we-have-to-specify-always-d-in


__SPARK
https://mageswaran1989.medium.com/spark-jargon-for-starters-af1fd8117ada

Spark does not support nested RDDs or performing Spark actions inside of transformations
https://issues.apache.org/jira/browse/SPARK-5063

https://nathankleyn.com/2017/12/29/using-transient-and-lazy-vals-to-avoid-spark-serialisation-issues/

https://www.linkedin.com/pulse/beyond-traditional-join-apache-spark-kirill-pavlov/
https://www.waitingforcode.com/apache-spark-sql/sort-merge-join-spark-sql/read

Optimizing Apache Spark SQL Joins: Spark Summit East talk by Vida Ha
https://www.youtube.com/watch?v=fp53QhSfQcI

https://stackoverflow.com/questions/34580662/what-does-stage-skipped-mean-in-apache-spark-web-ui

Шаблоны конфигов
https://github.com/apache/spark/tree/branch-3.4/conf

https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html

https://spark.apache.org/docs/latest/sql-data-sources.html
https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html

https://spark.apache.org/docs/latest/sql-data-sources-csv.html
https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option
https://spark.apache.org/docs/latest/sql-data-sources-json.html
https://spark.apache.org/docs/latest/sql-data-sources-text.html

https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option
https://spark.apache.org/docs/latest/sql-data-sources-orc.html

https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html
https://mvnrepository.com/artifact/org.postgresql/postgresql

https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html
https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html#spark-sql
https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-30

https://github.com/datastax/spark-cassandra-connector#documentation
https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md
https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector

https://github.com/RedisLabs/spark-redis/blob/master/doc/dataframe.md

https://www.mongodb.com/docs/spark-connector/upcoming/
https://www.mongodb.com/products/integrations/spark-connector
https://github.com/mongodb/mongo-spark


+https://spark.apache.org/docs/latest/rdd-programming-guide.html
+https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence

+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html
+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html

+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Row.html
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala

+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Column.html
+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/TypedColumn.html

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/index.html
https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/DateType.html
https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/TimestampType.html

https://spark.apache.org/docs/latest/sql-ref.html
https://spark.apache.org/docs/latest/sql-ref-datatypes.html#data-types

+https://spark.apache.org/docs/latest/api/sql
+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html
+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameNaFunctions.html
+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/RelationalGroupedDataset.html
+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/expressions/Window$.html
+https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/expressions/WindowSpec.html


__SQL
Оконные функции с «форточкой» или как пользоваться фреймом
https://habr.com/ru/companies/otus/articles/490296/


__SPARK_STREAMING
Spark Structured Streaming graceful shutdown — Что в этом сложного и как это правильно делать?
https://habr.com/ru/articles/569898/

https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html
https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources
https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks

https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10


__STATEFUL STREAMING
https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#state-store

https://medium.com/@vndhya/stateful-processing-in-spark-structured-streaming-memory-aspects-964bc6414346


__KAFKA
Apache Kafka — скоро без ZooKeeper
https://habr.com/ru/company/southbridge/blog/552688/

Алгоритм Raft
https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_Raft

https://kafka.apache.org/


__DELTA
https://docs.delta.io/latest/quick-start.html


__ELASTIC
https://www.elastic.co/
https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-dev-mode
https://www.elastic.co/guide/en/kibana/current/docker.html#_running_kibana_on_docker_for_development


__CASSANDRA
https://cassandra.apache.org/_/index.html
https://cassandra.apache.org/doc/stable/cassandra/architecture/dynamo.html#dataset-partitioning-consistent-hashing

https://thenewstack.io/an-apache-cassandra-breakthrough-acid-transactions-at-scale/
https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-15%3A+General+Purpose+Transactions

https://www.guru99.com/cassandra-query-language-cql-insert-update-delete-read-data.html


__JUPYTER_NOTEBOOK
jupyter notebook - запустить Jupyter Notebook

https://www.8host.com/blog/ustanovka-jupyter-notebook-dlya-python-3-v-ubuntu-20-04-i-podklyuchenie-po-ssh-tunnelyu/





Настройка Intellij IDEA для программы "Apache Spark для задач дата инжиниринга"
https://www.youtube.com/watch?v=Tf73EZbYMKM

Пример сборки fat-jar
https://github.com/MrSandmanRUS/ScalaAssemblyPluginExample

Запустить scala приложение
scala <scala.jar>

Поиск файла
locate spark-shell



QUESTION:
1.
клиент создает объект Distributed File System (?) - deprecated, теперь FileSystem вместо него?

create() on name node (проверка - возможно файл с таким именем уже существует + отдает список нод) (?) - не увидел проверки в коде - изучить более подробно

docker-compose для практики hdfs cli

2.
нет записи чата

3.
l_3.RDD
counterAccum иногда не равен rdd4.count() (?) - проблема в Idea?
в spark-shell - равен

4.
DataFrame_2
в плане нет struct + to_json (?)

5.
l_5.DataFrame_5 - В Spark 3+ - не работает: Found 0 WholeStageCodegen subtrees (?)

6.
l_6/Datasource_2.scala - spark.sql("SET -v")
WARN SQLConf: SQL configurations from Hive module is not loaded
+ build.sbt "spark-hive"

Размер будет больше ... (?) - размер получился меньше

#! Legacy index templates are deprecated in favor of composable templates - (?)
PUT _template/airports

8.
Watermark нужен только для stream-stream join (?)

Посмотреть лабу 4-b - стриминг

10.
В BigDataTools можно работать с удаленным кластером Spark - как (?)

Нет записи чата


__11

https://spark.apache.org/downloads.html

Список всех конфигураций спарка
https://spark.apache.org/docs/latest/configuration.html

Мониторинг спарк приложений (метрики/REST/...)
https://spark.apache.org/docs/latest/monitoring.html

YARN overcommitment
https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/OpportunisticContainers.html

YARN preemption
https://datacadamia.com/db/hadoop/yarn/preemption


__12

spark.locality.wait
https://spark.apache.org/docs/latest/configuration.html#scheduling




__1
4-00 -> 40-35 - введение
1-48-55 -> 2-01-25
2-12-15 -> 2-22-05 - HDFS cli demo
3-33-20


Под словом Hadoop могут подразумеваться:
1. core сервисы Hadoop (HDFS/YARN/MapReduce) - наиболее корректное определение
2. вся экосистема сервисов Hadoop (Spark/Hive/HBase/...)
3. конкретный вычислительный кластер под управлением Hadoop

Предпосылки появления Hadoop:
1. потребность в распределенных хранилищах
2. масштабирование вычислений
3. управление ресурсами

Модели обработки данных:
1. ETL = Extract Transform Load - DWH
+ выверенные и максимально детальные сущности
+ быстрые операции над сущностями
- необходимость поддерживать/дорабатывать модель данных
- при ETL трансформации скорее всего будут утеряны/изменены исходные данные
- более сложные трансформации (необходимо соответствовать модели данных)
2. ELT = Extract Load Transform - Data Lake
+ быстрое подключение источников

Архитектуры хранилищ:
1. DWH = schema on write - схема задается при записи данных
2. Data Lake = schema on read - схема задается при чтении данных - удобно хранить слабо/неструктурированные данных (text/csv/json/...)

Разделение schema on write/read не является строгим, в DWH есть стейджинговые зоны для хранения неструктурированных данных, в Data Lake - можно задавать схему при записи (пример - parquet)

Концепции DWH и Data Lake не являются взаимоисключающими - чаще всего DWH строится над Data Lake   по какой-либо модели (inmon/kimball/data vault/anchor model/...)

Core сервисы Hadoop:
1. HDFS = Hadoop Distributed File System - распределенная файловая система
2. YARN = Yet Another Resource Negotiator - менеджер ресурсов
3. MapReduce - фреймворк для распределенных вычислений

1. Data ingestion (поглощение) tools - инструменты импорта/экспорта данных между источниками и HDFS
  1.1 Flume - экспорт данных из источников в HDFS
  Альтернативы: NiFi/StreamSets
  1.2 Sqoop - импорт/экспорт данных между RDBMS (обычно из реплики - для снижения нагрузки на рабочую базу) и HDFS
  Альтернативы: Debezium/Kafka Connect
  1.3 Kafka - очередь с огромной пропускной способностью
  Альтернативы: RabbitMQ/Pulsar/Nuts/Kinesis (Amazon)/PubSub (Google)
2. On HDFS - HBase (wide column database)
3. On YARN - Spark/Flink/Storm (стриминг)/Lucene Solr (полнотекстовый поиск)/MapReduce
4. On MapReduce - Hive/Pig/Mahout
5. Zookeeper - metadata storage
   Используется вместе с HDFS/Kafka (переходит на Raft)/YARN/ClickHouse/HBase
6. Cassandra


Zookeeper - это централизованная служба (single source of truth) для поддержки информации о конфигурации, именования, обеспечения распределенной синхронизации и представления групповых служб
Zookeeper - это каталог, хранящий метаинформацию в z-нодах
Примеры: идентификатор мастер ноды/список воркеров/выполняющиеся таски/...

Линеаризуемость - события должны передаваться/обрабатываться в порядке их поступления
Линеаризуемость - одна из гарантий консистентности данных

Zookeeper - распределенный сервис (есть leader/follower ноды) - это обеспечивает отказоустойчивость и балансировку/масштабирование нагрузки

CAP теорема - система может соответствовать только 2-м критериям из CAP

1. CA - нераспределенная система
2. AP - распределенная система, главная цель - Availability
Пример - горячие кэши - лучше иметь дублирующиеся/отличающиеся кэши в разных частях системы, чем не иметь их вообще (тк будет большая нагрузка на БД)
3. CP - распределенная система, главная цель - Consistency
В случае возникновения неконсистентного состояния в части системы - эта часть (ноды в этой части) отключается, чтобы избежать неконсистентности данных (например split brain)

Zookeper - CP система
etcd/consul - системы близкие по назначению с Zookeeper


HDFS - файловая система, предназначенная для хранения файлов большого размера, поблочно распределенных между узлами вычислительного кластера

Элементы архитектуры HDFS:
1. Name node - хранит метаданные (маппинги имя файла -> список блоков/реплик/...) - единая точка отказа
2. Data node - хранит блоки данных
Увеличивая количество дата нод - масштабируем отказоустойчивость/скорость чтения и записи

Файл - это только запись в метаданных (метаданные хранятся в оперативной памяти) на name node 
Содержимое файла хранится в нескольких блоках одинакового размера (зависит от размера блока в HDFS) на дата нодах
Метаинформация о файле размером 50Кб будет занимать такой же объем в оперативной памяти нейм ноды, как и файл размером в дефолтный размер блока

Клиент выполняет чтение/запись напрямую на дата нодах (данные не перемещаются через нэйм ноду - у нее запрашиваются только метаданные) => большая пропускная способность

Поблочная запись обеспечивает параллельное чтение (не ограничиваемся пропускной способностью 1-й ноды) + равномерную утилизацию свободного места
Перекосы блоков на нодах исправляются балансировщиком (фоновый процесс)

Фактор репликации (работает на уровне блоков) по умолчанию = 3
HDFS умеет различать не только разные ноды, но и разные стойки (rack awareness) - топология конфигурируется (принадлежность блока к стойке) => одна из копий блока будет создаваться в другой стойке. Возможна настройка отказоустойчивости на уровне датацентра

Чтение из HDFS - Java API:
1. клиент создает объект Distributed File System (?)
2. getFileBlockLocations() from name node (конвертация имени файла в список блоков на нэйм нодах)
3. клиент создает FSDataInputStream
4. чтение данных из дата нод
5. FSDataInputStream.close

Запись на HDFS - Java API:
1. клиент создает объект Distributed File System (?)
2. create() on name node (проверка - существует ли файл с таким именем + отдает список нод) (?)
3. клиент создает FSDataOutputStream
4. запись данных - write packet + ack packet
5. репликация (фоновая) записанных блоков между дата нодами - write packet + ack packet
6. FSDataOutputStream.close
7. complete on name node

High Availability (HA) HDFS cluster - используется 2 нэйм ноды (active + standby)
В z-ноде зукипера хранится id активной нэйм ноды

3. Secondary Name node - нэйм нода передает ей fsimage и логи, отбратно возвращается fsimage c вмерженными изменениями из логов
4. ZKFC (Zookeeper Failover Controller) - подключены к active/standby нэйм нодам с одной стороны и к ZK Cluster с другой
ZKFC определяют какая нэйм нода является активной в данный момент
5. Journal node
Активная нэйм нода посылает на кластер с журнальными нодами (shared storage of journal nodes) изменения. Standby нэйм нода вычитывает изменения из кластера журнальных нод

Name node federation

Если используется HA HDFS кластер - клиенту нужно подложить конфиги (hdfs-site.xml/core-site.xml из /etc/hadoop/conf) в resources - active нэйм нода будет определяться автоматически
Путь к файлам конфигурации HDFS/YARN - /etc/hadoop/conf

путь к объекту: hdfs://name_node:port/object_path - если нейм нода одна (не HA)

Особенности HDFS:
1. не любит мелкие файлы - нагрузка на ram в нэйм ноде (1 файл = 1 запись в ram нэйм ноды)
=> желательно репартицировать ~под размер блока
2. не поддерживает редактирование файла - исходный файл удаляется и записывается новый
3. работает на дешевом железе

HDFS API:
1. libhdfs - C API
2. FileSystem - нативное Java API
3. WebHDFS/HttpFS - REST API
4. HDFS CLI - hdfs dfs -<command>

При удалении из HDFS - файлы сначала помещаются в .Trash (т.е. можно отменить удаление - переместив их из .Trash) - затем автоматически (через ~24 часа) удаляются

HDFS - это object storage

Типы хранилищ:
1. Object storage - работают по своим протоколам/api, а не на уровне фс
Упрощенно - key (path): value (object)
Object storage менее производительны, но являются более универсальными/масштабируемыми чем block storage
2. Block storage - работают на уровне фс - ~большой диск
3. File storage

Oбъектные хранилища:
1. HDFS
2. Minio - больше подходит для хранения мелких файлов, чем HDFS
Проще конфигурируется + хорошо разворачивается в kubernetes
(S3 - Amazon/GCS - Google)
3. Ceph - более универсален и производителен, чем HDFS, но гораздо сложнее в настройке/поддержке
4. GlusterFS
5. SeaweedFS - для хранения очень маленьких файлов
6. Apache Ozone - развитие HDFS


Системы контейнеризации - отвечают за управление ресурсами и изоляцию работающих процессов:
1. YARN
2. Kubernetes
3. Mesos

YARN - модуль, отвечающий за управление ресурсами кластера (v2 - ram/cpu, v3 +gpu) и планирование заданий

Resource manager - управляет ресурсами кластера
Node manager - управляет ресурсами конкретной ноды (физической машины)

Механизм запуска приложения в YARN:
1. Client -(job submission - .jar)-> Resource manager
2. Resource manager -> Node manager -> разворачивается контейнер Application master
3. Application master -(resource request)-> Resource manager (success) -> Node manager -> разворачиваются контейнеры для обработки задачи (воркеры)

Обмен сообщениями:
1. Воркер -(work status)-> Application master
2. Node manager -(node status)-> Resource manager

В YARN используется двухэтапный планировщик ресурсов:
1. клиент при запуске приложения указывает необходимые ресурсы (этап job submission)
2. приложение (Application master) может запрашивать дополнительные/отдавать освободившиеся ресурсы (пример - dynamic allocation в Spark)

Kubernetes имеет одноэтапный планировщик

Контейнер - это отдельный jvm процесс
Если контейнер превысил допустимый порог потребления по памяти - YARN убивает контейнер
Если Application master падает - все дочерние контейнеры уничтожаются

YARN использует механизм очередей - указывается приоритет + выделяемые ресурсы (MIN/MAX) + логика планирования (FIFO/FAIR):
1. отдельная очередь для долгих тяжелых задач
2. отдельная очередь для ad-hoc запросов
3. отдельная очередь для стриминговых задач
4. отдельная очередь для обучения моделей
5. отдельная очередь на каждый отдел
...

Если приложение долго висит в статусе accepted - кластеру не хватает ресурсов для создания воркеров

host:8088 - YARN web UI
yarn -kill <yarn_id> - убить ярн процесс из консоли
spark submit ... --queue <queue_name> - указание очереди для ярна


MapReduce:
1. подход к организации распределенных вычислений, предложенный Google
2. фреймворк в составе Hadoop

Главная идея MapReduce - вычисления можно представить как последовательность операций:
1. Map - трансформация
2. Shuffle - перераспределение данных
3. Reduce - агрегация

MapReduce стабильнее, но гораздо медленнее Spark


Hive - SQL движок, для выполнения запросов к слабоструктурированным данным (запрос преобразуется в MapReduce джобу)
Для запросов используется язык HiveQL
Hive имеет свой metastore для хранения метаинформации (имя -> структура таблицы)

Этапы выполнение запроса в Hive:
1. 
JDBC/ODBC/Thrift -(HQL)-> HiveServer2
CLI/WebUI -(HQL)-> Driver
->
Hive:
2.1 HiveServer2
2.2 Driver
2.3 Query parser (compiler) + optimizer + planner
2.4 Metastore DB (RDBMS) - хранит маппинги имя таблицы - путь в фс
2.6 Execution engine
->
3. MapReduce job on Hadoop/YARN

Hive может использовать движки Spark/Tez (работают быстрее), вместо MapReduce

Партицирование - разбиение данных на уровне файловой системы
Партицирование - ускоряет получение данных (~индекс в реляционных бд)

SHOW CREATE TABLE <table_name> - посмотреть DDL таблицы (в т.ч. информация о партицировании)

Виды форматов для хранения данных:
1. Колоночные
ORC/Parquet - последовательное чтение (гораздо быстрее случайного чтения с диска) нужных колонок + отличное сжатие (однородные данные сжимаются лучше) + индексы/метаинформация
Нельзя записывать построчно => не подходят для потоковой записи
Лучший формат для аналитики
2. Строковые
Avro - бинарный формат (компактное хранение данных) + схема не дублируется в каждом сообщении
Avro - лучший выбор для потоковой записи

В Hive можно описать собственный формат сериализации/десериализации - ROW FORMAT SERDE <class>

Типы таблиц Hive:
1. Internal (внутренняя) - при дропе таблицы удаляются данные и метаданные 
2. External (внешняя) - при дропе таблицы удаляются только метаданные, данные остаются

Если во внешней партицированной таблице Hive после записи новой партиции новых данных не видно (запрос hql):
MSCK REPAIR TABLE <table_name> - обновление метастора (перечитает список партиций)
+ INVALIDATE METADATA - если используется Impala

Варианты написания запросов в Hive:
1. Beeline (deprecated)/Hive CLI
2. HUE (Human User Experience) - Web UI
3. библиотеки - JDBC/pyhive/...

В Hive нельзя делать выборочные (WHERE) UPDATE/DELETE - т.к. данные - это файлы на HDFS
Для OLAP нагрузки характерен принцип Append Only

В Hive можно строить ACID таблицы в формате ORC - производительность очень низкая (можно использовать для небольших таблиц)

В случае необходимости ACID для больших таблиц (например - таблица фактов) лучше использовать Delta/Iceberg/Hudi - записываются файлы с дельтами относительно предыдущего состояния (версионируемость) + логи транзакций + фоновый компакшн




__2
https://github.com/Gorini4/jupyter_scala_docker
6-00
53-05 -> 59-30
1-49-25 -> 1-59-00
3-01-30 -> 3-08-30
3-25-20


Scala:
1. объектно-ориентированный
2. функциональный
3. статически-типизированный
4. работает поверх JVM
5. можно использовать Java библиотеки

Scala фреймворки:
1. анализ данных и ETL - Spark
2. потоковая обработка - Flink
3. распределенные приложения - Akka
4. параллельные и асинхронные вычисления - Monix/CE/ZIO

Консоль:
1. Scala
2. Ammonite

Онлайн компиляторы:
1. Scastie
2. ScalaFiddle

IDE:
1. IntelliJ IDEA + Scala plugin
2. VSCode + Metals/...

Типы данных:
1. Int/Double/Char/...
2. Tuple<N>
3. Immutable collections - String/List/Set/Map/...
4. Mutable collections
5. Option/Try
6. Unit ()

Иерархия коллекций:
1. Traversable
2. Iterable
  2.1 Seq
    2.1.1 IndexedSeq - Array/Range/String/Vector/StringBuilder/ArrayBuffer - индексы
    2.1.2 LinearSeq - List/Queue/Stack/Stream/LinkedList/MutableList - связи между элементами
    2.1.3 Buffer - ArrayBuffer/ListBuffer
  2.2 Map
  2.3 Set

null - нет значения (Java)
None - отсутствующее значение (подтип Option)
Nil - пустой список (подтип List)
Nothing - подтип всех типов

dataset.map(el => myCustomTransform(el)) - это выражение будет одинаковым для:
1. Scala коллекций
2. параллельных Scala коллекций
3. Spark Datasets
4. Flink DataStreams
5. потоков сообщений Akka

Иерархия типов:
1. Any
  1.1 AnyVal
    1.1.1 Double
    1.1.2 Float
    1.1.3 Long
    1.1.4 Int
    1.1.5 Short
    1.1.6 Byte
    1.1.7 Boolean
    1.1.8 Char
    1.1.9 Unit
      1.1.x.1 Nothing
  1.2 AnyRef (= java.lang.Object)
    1.2.1 List
    1.2.2 Option
    1.2.3 Other classes ...
      1.2.x.1 Null
        1.2.x.1.1 Nothing
        
Элементы ООП в Scala:
1. Class
2. Object
3. Trait

Case class (СС):
1. удобны для моделирования неизменяемых данных
2. сравниваются по значению полей, а не по ссылке
3. имеют метод copy - для быстрого копирования неизменяемых объектов
4. не нужно ключевое слово new при создании
5. Companion Object из коробки - apply/unapply/toString/equals/hashCode
7. extends Serializable из коробки
8. Immutable

СС широко используются для сериализации и описания данных:
1. JSON-библиотеки поддерживают автоматическое кодирование/декодирование CC
2. Spark использует CC для описания типизированных наборов данных - Datasets
3. Akka Typed

Object:
1. является Singleton - может существовать только 1 экземпляр
2. используется для размещения методов, которые могут быть вызваны без создания экземпляра класса (= Java static method)
3. Scala App - это объект

Companion object (CO):
1. объект с тем же именем, что и класс/трейт
2. должен быть определен в том же файле
3. может обращаться к private методам/полям компаньона

СО применяется для:
1. размещения конструкторов
2. размещения ~статических методов
3. ...

Trait:
1. = Java Interface 
2. основная задача - определить контракт на реализацию некоторой функциональности
3. нет конструктора
4. можно подмешивать неограниченное количество трейтов

Abstract class:
1. способ описать общие свойства и позволить потомкам реализовать их с помощью собственной логики
2. можно наследовать только 1 класс

Generic class - класс с параметризированным типом. Параметризация позволяет создавать классы с общей функциональностью для разных типов

Модификаторы доступа:
Модификатор	Класс	Компаньон	Подкласс	Пакет	Мир
no modifier	  +	    +		    +		  +	 +
protected	  +	    +		    +		  -	 -
private	  +	    +		    -		  -	 -

Анонимный класс - позволяет реализовать/расширить класс "на лету"

1. final классы не могут быть расширены
2. sealed классы могу быть расширены только в том же файле

Implicit class (IC) - использутся для неявного расширения функционала других классов

Неявные преобразования применяются в двух случаях:
1. если выражение e имеет тип S, а S не соответствует выражениям предполагаемого типа T.
В этом случает ищется преобразование c, которое применимо к e, и тип результата которого соответствует T
2. при вызове e.m, где экземпляр e имеет тип S, но у типа S отсутствует метод m.
В этом случае ищется преобразование c, которое применимо к e, и результат которого содержит член с именем m

Полезные библиотеки:
1. парсинг Json:
  1.1 Json4s
  1.2 Circe
  библиотеки позволяют использовать разные движки - например Jackson
2. работа с конфигурацией:
  2.1 Typesafe config
  2.2 PureConfig
3. тестирование
  3.1 ScalaTest




__3
1-40
59-55 => 1-04-45
1-40-10 => 1-47-05
3-00-40


Spark - фреймворк для создания распределенных приложений для обработки данных
Spark является эволюцией фреймворка Hadoop MapReduce

Области применения:
1. распределенная обработка больших данных
2. построение ETL пайплайнов
3. работа со структурированными данными (SQL)
4. разработка стриминговых приложений

Архитектура:
1. Driver (запускается в Appication Master в YARN) - всегда 1 на джобу - может быть запущен в кластере (cluster)/вне кластера (client)
  1.1 предоставляет API через SparkSession/SparkContext
  1.2 выполняет код - скомпилированный jar (или py)
  1.3 управляет выполнением задачи
  1.4 не занимается обработкой данных
2. Executor (запускаются на Worker нодах YARN)
  2.1 обрабатывает данные
  2.2 каждый executor работает со своим сегментом данных - Partition
  2.3 код на экзекьюторе не выполняется напрямую - через API трансформации передаются с драйвера  на экзекьюторы и там применяются
  2.4 получает задачи от драйвера
3. Cluster Manager (Kubernetes/YARN/Mesos/Standalone)
  3.1 отвечает за аллокацию контейнеров, выполняющих код драйвера/экзекьюторов на кластере
  3.2 квотирует ресурсы между пользователями
  3.3 контролирует состояние контейнеров

Driver/Executor'ы - обычные jvm приложения (могут быть многопоточными - если выделить несколько ядер)
SparkContext - точка входа в API - отвечает за загрузку конфигурации/запуск экзекьюторов/... 

1 Task = 1 Partition = 1 Cpu core
1 таска - это обработка 1-ой партиции 1-м ядром

Компоненты Spark DAG (Directed Acyclic Graph):
1. Job
2. Stage - разные стэйжди разделяются shuffl'ом
3. Task - процесс обработки 1-ой партиции 1-м ядром

RDD (Resilient Distributed Dataset) - типизированная неизменяемая неупорядоченная партицированная коллекция данных, распределенная по узлам кластера
RDD - самая базовая/низкоуровневая структура в Spark доступная разработчику

RDD API - низкоуровневое API, которое позволяет применять любые функции к распределенным данным
При использовании RDD API обработка всех исключительных ситуаций лежит на плечах разработчика

RDD можно создать из:
1. локальной коллекции на драйвере
2. файла - в локальной/распределенной файловой системе (например HDFS)
3. базы данных

Виды операций с RDD:
1. Transformation (трансформации) - map/filter/...
2. Action (действия) - reduce/collect/take/count/foreach/...

Transformations:
1. всегда создают новый RDD и не изменяют исходный
2. всегда являются ленивыми (lazy) - создают граф, но не запускают его вычисление
Гораздо эффективнее оптимизировать/выполнить граф целиком, чем выполнять трансформации по одной
3. иногда (часто) неявно требуют перемещения данных между экзекьюторами - shuffle

Actions:
1. выполняют действия над RDD
2. запускают граф вычислений

RDD - неизменяемы, трансформации всегда создают новый RDD (не изменяют исходный)
Элементами RDD могут быть коллекции/кейс классы/кортежи/...

Обычный экзекьютор - 1/2 ядра + 4/8 Gb ram
Большое количество памяти на экзекьюторе => большая нагрузка на GC

Партиция - это Iterator (те поэлементная обработка), но, например при записи, его необходимо материализовать в памяти целиком
Партиции обрабатываются параллельно, данные внутри партиии обрабатываются последовательно
Количество партиций/количество элементов в партиции в RDD/DF зависит от имплементации конкретного источника (определяет разработчик коннектора)


PairRDD - расширенный класс функций, доступных для RDD, где элементы - это кортеж из 2-х элементов (key, value)

Join позволяет соединить два PairRDD по ключу
PairRDD поддерживают join(inner), leftOuterJoin, fullOuterJoin

При работе с RDD API - функции сериализуются на драйвере - передаются на каждый экзекьютор - десериализуются и применяются

Option[T] - монада, позволяющая работать с отсутствующими данными, избегая исключительных ситуаций при работе null. Одним из ее преимуществ является то, что ее можно рассматривать как коллекцию, что позволяет применить к RDD[Option[T]] метод flatMap, который вернет RDD[T], убрав все None из датасета

Результат shuffle - конкретные файлы на файловой системе воркеров

В local mode - нет экзекьюторов, все происходит внутри драйвера => нет сериализации/десериализации




__4
1-00
42-25 => 48-00
1-52-40 => 1-58-30
2-51-35


!!! RDD в Python работаю крайне медленно

RDD API vs DF API:
1. Типы данных
RDD: низкоуровневая распределенная коллекция данных ЛЮБОГО типа
DF: таблица со схемой, состоящей из колонок ОПРЕДЕЛЕННЫХ типов, описанных в org.apache.spark.sql.types

2. Обработка данных
RDD: сериализация (Java serializer) функций на драйвере -> десериализация и применение на экзекьюторах
DF: кодогенерация (Thungsten) -> Java код трансформации (для каждого физического оператора) -> компилятор janino -> применение скомпилированного кода к данным

3. Функции и алгоритмы
RDD: нет ограничений
DF: ограничен SQL операторами, функциями org.apache.spark.sql.functions и пользовательскими функциями udf

4. Источники данных
RDD: каждый источник имеет свое API
DF: единое API для всех источников - интерфейс DataSource v1/v2

5. Производительность
RDD: напрямую зависит от качества кода
DF: встроенные механизмы оптимизации SQL запроса - Catalyst

6. Потоковая обработка данных
RDD: устаревший DStream
DF: активно развивающийся Structured Streaming

На текущий момент RDD является низкоуровневым API, которое постепенно уходит "под капот" Apache Spark
DF API представляет собой библиотеку для обработки данных с использованием SQL примитивов

Создать DF можно из:
1. локальных коллекций
2. файлов
3. баз данных

1. методы filter/select принимают в качестве аргументов колонки org.apache.spark.sql.Column
Это может быть либо ссылка на существующую колонку, либо функция из org.apache spark.sql.functions
2. любые трансформации возвращают новый DF, не изменяя существующий
3. тип org.apache.spark.sql.Column играет важную роль в DF API - на его основе создаются ссылки на существующие колонки, а также функции, принимающие org.apache.spark.sql.Column и возвращающие org.apache.spark.sql.Column
По этой причине обычное сравнение == не будет работать в DF API, т.к. filter принимает org.apache.spark.sql.Column, а не Boolean
4. класс DataFrame начиная со Spark 2 представляет собой org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] => его описание следует искать в org.apache.spark.sql.Dataset


Очистка данных:
DF API содержит класс функций "Not Available", описанный в пакете org.apache.spark.sql.DataFrameNaFunctions

1. na.drop - удаление строк с null
2. na.fill - заполнение значений null
3. na.replace - замена значений

Общая концепция - избавиться от null т.к.:
1. в Spark - null + 1 = null
2. udf и ряд встроенных функций уязвимы к null

distinct - получает уникальные записи для всех колонок датафрейма
dropDuplicates - позволяет указать колонки, по которым будут получены уникальные записи

DF API обладает удобными методами для очистки данных, позволяющими разработчику сконцентрироваться на бизнес логике, а не на написании функций для обработки возможных исключительных ситуаций


Агрегации:
agg - позволяет использовать любые aggregate functions из пакета org.apache.spark.sql.functions

collect_list/collect_set/pivot - "тяжелые" агрегаты

Используя методы struct и to_json мы можем превратить произвольный набор колонок в JSON строку - часто используются перед отправкой данных в Kafka

pivot - создает колонки из значений заданной колонки

DF API позволяет строить большое количество агрегатов. Операции groupBy/cube/rollup возвращают org.apache.spark.sql.RelationalGroupedDataset, к которому затем необходимо применить одну из функций агрегации - count/sum/agg/...

При вычислении агрегатов необходимо помнить, что эта операция требует перемещения
данных между экзекьюторами, что, в случае перекошенных данных, может привести к OOM на экзекьюторе


Кэширование:
По умолчанию при применении каждого action Spark пересчитывает весь граф, что негативно сказывается на производительности приложения

cache/persist/repartition - позволяют не вычислять граф несколько раз
Механизм кэширования работает одинаково для RDD/DF

cache/persist - сохраняют состояние графа после первого действия, и следующие действия обращаются к закэшированным данным 
persist - позволяет выбрать, куда сохранить данные
cache - использует значение по умолчанию - в текущей версии Spark это StorageLevel.MEMORY_ONLY

Важно помнить, что данный кэш не предназначен для обмена между разными Spark приложениями - он является внутренним для конкретного приложения. После окончания работы с данными необходимо выполнить unpersist для очистки памяти

Кэш в Spark - LRU, т.е. перезаписывает наименее используемые данные

Использование cache/persist позволяет существенно сократить время обработки данных, однако следует помнить об увеличении потребляемой памяти на экзекьюторе

StorageLevel.MEMORY_ONLY - при заполнении кэша, новые данные будут затирать старые
StorageLevel.MEMORY_AND_DISK - при заполнении кэша в памяти, новые данные будут вытеснять старые на диск. Кэш на диске не перетирается новыми данными => место на диске может закончится


Репартицирование:
RDD и DF представляют собой классы, описывающие распределенные коллекции данных
Эти коллекции разбиты на крупные блоки, которые называются партициями

В Spark DAG (Direct Acyclic Graph) есть 3 основные компонента:
1. Job - представляет собой весь граф целиком, от момента создания DF до применения action к нему Job состоит из одной или более stage
2. Stage - возникает когда необходимо сделать shuffle
Stage состоит из task
3. Task - базовая операция над данными
Одновременно Spark выполняет N task, которые обрабатывают N партиций, где N - это суммарное число доступных ядер на всех экзекьюторах

Как следствие - важно обеспечить:
1. достаточное количество партиций для распределения нагрузки по всем экзекьюторам
Количество партиций нужно делать кратно больше числа ядер (x2, x3, x4)
Эмпирический коэффициент = 3 (например 100 ядер на 300 партиций)
2. равномерное распределение данных между партициями

spark.conf.set("spark.sql.shuffle.partitions", <partition_number>) - подбирается индивидуально исходя из ресурсов кластера (по умолчанию = 200)

!!! Передача данных между стейджами (шафл) выполняется через дисковую подсистему
=> репартиционирование считается дорогой операцией

Алгоритм выполнения репартицирования:
1. в каждой партиции выполняется сортировка данных по ключам
2. на драйвере формируется карта расположения ключей по партициям на экзекьюторах
3. каждый экзекьютор открывает фетч-реквесты к другим экзекьюторам, получает нужные данные
и сохраняет их на свою файловую систему
Результат репартиционирования - файлы на дисках воркеров
4. возникает 2-й стэйжд (например повторный HashAggragate) - в рамках которого выполняется чтение данных c файловой системы. Далее работа производится в памяти

Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников


DataSkew при агрегациях:
Поскольку при вычислении агрегата происходит неявный HashPartitioning по ключам агрегата, то при выполнении определенных условий происходит нехватка памяти на экзекьюторе, которую нельзя исправить, не изменив подход к постороению агрегата

Одно из решений - соление ключей. Оно позволяет существенно снизить объем данных в перекошенных партициях. При key salting агрегировать нужно дважды (на втором этапе нужно каким-то образом объединить агрегаты, полученные на первом этапе), но при этом распределение данных по партициям будет более равномерным, что позволит избежать OOM на экзекьюторе

Не любую задачу можно решить с помощью key salting - например distinct count
Проблему слишком больших партиций можно попробовать решить разбиением исходного датасета на более мелкие

1. партицирование - важный аспект распределенных вычислений, от которого напрямую зависит стабильность и скорость вычислений
2. в Spark всегда работает правило 1 TASK = 1 CORE = 1 PARTITION
3. репартицирование и соление данных позволяет решить проблему перекоса данных
4. репартицирование использует дисковую и сетевую подсистемы (при использовании стандартного шафл-сервиса) - обмен данными происходит по СЕТИ, а результат записывается на ДИСК, что может стать узким место при выполении репартицирования


Встроенные функции:
Помимо базовых SQL операторов в Spark существует большой набор встроенных функций:
1. методы из spark.sql.functions
2. SQL built-in functions

1. Spark обладает широким набором функций для работы с колонками разных типов:
стороки/числа/.../словари/массивы/структуры
2. встроенные функции принимают колонки org.apache.spark.sql.Column и возвращают org.apache.spark.sql.Column в большинстве случаев
3. встроенные функции можно и нужно использовать вместе - на вход встроенных функций могут подаваться результаты других встроенной функции, т.к. все они возвращают sql.Column


Пользовательские функции:
UDF (user defined function) - если функционала встроенных функций не хватает, могут принимать до 16 аргументов

null в Spark превращается в null внутри UDF

Пользовательская функция может возвращать:
1. простой тип: String/Long/Float/Boolean/... (Scala type)
2. массив - любые коллекции, наследующие Seq[T] - List/Vector/...
3. словарь - Map[A, B]
3. case class
4. Option[T] (None = null in DF)

1. пользовательские функции позволяют реализовать произвольный алгоритм и использовать его в DF API
2. пользовательские функции работают медленнее встроенных, поскольку при использовании встроенных функций Spark использует ряд оптимизаций (например векторизацию вычислений на уровне CPU) + Catalyst + Thungsten + отсутствуют лишние сериализации/десериализации/копирование


Соединения:
Позволяют соединять два DF в один по заданным условиям

Виды join'ов по типу условиЙ:
1. equ-join - соединение по равенству одного или более ключей
2. non-equ join - соединение по условию, отличному от равенства одного или более ключей

Виды join'ов по методу соединения:
1. Inner
2. Left Outer
3. Right Outer
3. Full Outer
4. Cartesian (Cross)
5. Left Anti
6. Left Semi

Условия соединения можно задать с помощью:
1. Seq(String)
2. sql.Column

Условие соединения можно задать с помощью Seq[String] - это синтаксический сахар, позволяющий не переименовывать колонки датасетов, а просто указать, что соединение будет производиться по колонкам с именами, входящими в seq

В более общем виде условие джойна должно быть выражено в виде sql.Column
Например: col("left_a") === col("right_a") - в выражении допускается использование встроенных функций, пользовательских функций и операторов сравнения. Следует помнить, что выполняется джоин двух распределенных датасетов и если условие соединения будет составлено плохо - Spark выполнит cross join производительность которого крайне мала

При джоине может производится репартицирование


Оконные функции:
Позволяют выполнять функции над "окнами" данных - при этом не теряются исходные данные (как в обычных агрегациях - например groupBy)

Окно создается из класса org.apache.spark.sql.expression.Window с указанием:
1) колонок, определяющих границы окон - partitionBy
2) колонок, определяющих порядок сорировки внутри окна - orderBy

Окно определяется списком колонок и сортировками
val window: WindowSpec = Window.partitionBy("a", "b").orderBy("a")

Применяя окна, можно использовать такие полезные функции из org.apache.spark.sql.functions как
lag/lead, а также эффективно работать с time-series данными

!!! Применение оконных функций приводит к shuffle
!!! row_number().over(Window.partitionBy()) = Exchange single partition (антипаттерн) => OOM

Use-cases оконных функций:
1. агрегатные функции
2. кумулятивные функции
3. вычисления над значениями одной колонки из разных строк (c orderBy т.к. это гарантирует сортировку)
4. нумерация строк в рамках партиции 
5. функции по плавающим окнам для time-series данных - rowsBetween/rangeBetween


Misc:
spark.read.json - позволяет читать не только файлы, но и Dataset[String], содержащие JSON строки
spark.read.csv - позволяет читать заархивированные csv-файлы

Методы map/flatMap - это переход к DS API => потеря перформанса из-за дополнительных действий:
1. десериализация (Internal row -> Java object)
2. MapPartition и применение функции
3. сериализация (Java object -> Internal row)

В Spark 3 появилась возможность написания плагинов для кастомных операторов (примеры - Direct Cassandra Join, Spark GPU)




__5
55-25 => 59-50
1-39-40 => 1-44-35

2-42-00


Планы выполнения задач:
Любой job в Spark SQL имеет под собой план выполнения, который генерируется на основе написанного кода. План запроса содержит операторы, которые потом превращаются в Java код. Поскольку одну и ту же задачу в Spark SQL можно выполнить по-разному, полезно смотреть в планы выполнения, чтобы:
1. убрать лишние shuffle
2. убедиться, что тот или иной оператор будет выполнен на уровне источника (predicate pushdown), а не внутри Spark
3. понять, какой вид join'а будет использован

Планы выполнения доступны в 2-х видах:
1. метод explain() у DF
2. на вкладке SQL/DataFrame в Spark UI

Варианты реализации физических операторов:
1. с кодогенерацией - представлены конкретным Java кодом, который компилируется janino
2. без кодогенерации - представлены кодом на scala (например - toJson)

Планы Spark:
1. Parsed Logical - запрос, распарсенный каталистом в некоторое дерево
2. Analyzed Logical - производятся валидации - колонки, указанные в select, есть в датафрейме/проверка типов/...
3. Optimized Logical - оптимизированное дерево (например - перестановка физических операторов)
4. Physical - набор физических операторов, строится на основании Optimized Logiсal

show = физический оператор CollectLimit

Любой датафрейм под капотом представляет собой RDD[InternalRow]
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.rdd.RDD

Имплементации InternalRow:
1. GenericInternalRow - человекочитаемые данные, располагаются в хипе
package org.apache.spark.sql.catalyst.expressions -> BaseGenericInternalRow
class GenericInternalRow(val values: Array[Any]) extends BaseGenericInternalRow
2. UnsafeRow - массив байт с некоторой метаинформацией, располагаются в оффхипе
package org.apache.spark.sql.catalyst.expressions -> UnsafeRow
public final class UnsafeRow extends InternalRow implements Externalizable, KryoSerializable

1. Spark составляет физический план выполнения запроса на основании написанного кода
2. при изучении плана запроса, можно понять, какие операторы будут применены в ходе обработке данных
3. план выполнения запросов - один из основных инструментов оптимизации запроса


Оптимизация соединений и группировок:
При выполнении join двух DF важно следовать рекомендациям:
1. фильтровать данные до join'а
2. использовать equ-join
3. если можно путем увеличения количества данных применить equ-join вместо non-equ join'а - то делать именно так
4. избегать cross-join
5. если правый DF помещается в память экзекьютора использовать broadcast join

Виды соединений:
1. BroadcastHashJoin - самый быстрый:
  1.1 equ-join
  1.2 using broadcast (в Hive называется map side join)
2. SortMergeJoin:
  2.1 equ-join
  2.2 sortable keys
3. BroadcastNestedLoopJoin:
  3.1 non-equ join
  3.2 using broadcast
4. CartesianProduct (cross join):
  4.1 non-equ join
5. HashJoin - был отключен по умолчанию в Spark ~2.2
Можно включить, но SortMergeJoin - эффективнее

Выбор алгоритма основывается на условии соединения и размере датасетов


BroadcastHash Join:
1. работает, когда условие - равенство одного или нескольких ключей
2. работает, когда один из датасетов небольшой и полностью вмещается в память экзекьютора
3. оставляет левый датасет как есть
4. копирует правый датасет на каждый экзекьютор (должен влезать в хип экзекьютора)
Копирование происходит по peer-to-peer алгоритму
5. составляет HashMap из правого датасета, где ключ - кортеж из колонок в условии соединения
6. итерируется по левому датасету внутри каждой партиции и проверяет наличие ключей в HashMap
7. может быть использован автоматически, либо явно - broadcast(df)


BroadcastTimeoutException (по умолчанию 300с) - на экзекьюторе не хватает памяти для броадкаста датафрейма

Указание объема памяти на экзекьюторах:
1. spark-submit: --executor-memory 4g
2. spark-submit: --conf spark.executor.memory=4g
3. spark-defaults.conf: spark.executor.memory 4g


SortMerge Join - дефолтный вид джоина больших датафреймов:
1. работает, когда ключи соединения в обоих датасетах являются сортируемыми
2. репартицирует оба датасета в 200 партиций по ключам соединения
3. сортирует партиции каждого из датасетов по ключам соединения
4. сравнивает левый и правый ключи, обходит каждую пару партиций и соединяет строки с одинаковыми ключами

SortMerge Join - упадет, если есть перекошенные партиции, не влезающие в память экзекьютора


BroadcastNestedLoop Join:
1. работает, когда один из датасетов небольшой и полностью вмещается в память экзекьютора
2. оставляет левый датасет как есть
3. копирует правый датасет на каждый экзекьютор
4. проходится циклом по каждой строке партиции левого датасета + вложенный цикл для обхода правого датасета и проверяет условие соединения
5. может быть использован автоматически, либо явно через broadcast(df)


CartesianProduct - стоит избегать:
1. создает пары из каждой партиции левого датасета с каждой партицией правого датасета, релоцирует каждую пару на один экзекьютор и проверяет условие соединения
2. на выходе создает N*M партиций
3. работает медленнее остальных и часто приводит к OOM на экзекьюторе

Признак CartesianProduct в UI - огромное количество партиций


Снижение объема shuffle:
В ряде случаев можно уйти от лишних shuffle операций при выполнении соединения
Для этого оба DF должны иметь одинаковое партицирование = одинаковое количество партиций и ключ партицирования, совпадающий с ключом соединения

Оптимизация видна в Spark UI:
1. ++ в плане выполнения - в SQL/Dataframe + Details
2. +  в графе выполнения - в Jobs
3. +  в explain видно, что 2 HashAggregate выполняются без репартицирования

Optimal number of spark.sql.shuffle.partitions = num_workers * cores_per_worker * 3 (или 4)

!!! После репартиционирования не имеет смысла делать cache т.к. данные уже будут лежать на дисках воркеров (т.к. результат репартиционирования - файлы)
т.е. репартицирование ~= persist(StorageLevel.DISK_ONLY)

После репартицирования по ключам партиции левого и правого датафрейма с одинаковыми ключами гарантированно будут в разных партициях, однако гарантии попадания на один и тот же воркер у этих партиций нет, запускается неявная операция релокации партиций на воркер


Управление схемой данных:
Типы колонок DF API:
1. скалярные - StringType/IntegerType/...
2. массив - ArrayType(T)
3. словарь - MapType(K, V)
4. структура - StructType

Вложенность словарей, массивов и структур не ограничена

Схема DF описывается классом StructType[Seq(StructField)]

StructField имеет 3 атрибута:
1. name
2. dataType
3. nullable

Иерархия типов:
1. DataType - абстрактный класс
  2. StructType
  3. ArrayType
  4. MapType
  5. AtomicType - protected[sql] - абстрактный класс для скалярных типов:
  StringType/NumericType/TimeStampType/DateType
    6. NumericType - абстрактный класс:
    IntegralType/FractionalType
      7. IntegralType - private[sql]:
      IntegerType/LongType
      8. FractionalType - private[sql]:
      DoubleType/FloatType

Схема используется:
1. при чтении источника - значительно ускоряет чтение (в худшем случае с автовыводом схемы - потребуется прочитать весь датасет)
2. при работе с Json/Avro - from_json/...


Оптимизатор запросов Catalyst:
Catalyst выполняет оптимизацию запросов с целью ускорения их выполнения и применяет следующие оптимизации:

1. Column projection - позволяет избегать чтения ненужных колонок при работе с источником
+ позволяет избегать вычисления ненужных колонок (например - для groubBy)
Работает для колоночных форматов - parquet/orc
Физический оператор чтения файлов - FileScan <format> - до сих пор работает на Datasource API V1
!!! Для неколоночных форматов (например - json) - ReadSchema[] будет указана в плане выполнения, но работать оптимизация не будет

2. Partition pruning - позволяет избегать чтения ненужных партиций. Работает только с файловыми форматами (json/orc/parquet/csv/text)
Best practice партиционирования - год/месяц/день

3. Predicate pushdown - позволяет "протолкнуть" условия фильтрации данных на уровень datasource 
В Datasource API V1 фильтр применяется дважды:
  1. на источнике - PushedFilters
  2. в спарке - Filter:
      // Filter (isnotnull(iso_region#46) AND (iso_region#46 = RU))
      *(1) Filter (isnotnull(iso_region#46) AND (iso_region#46 = RU))
      +- *(1) ColumnarToRow
         // PushedFilters: [IsNotNull(iso_region), EqualTo(iso_region,RU)]
         +- FileScan parquet [ident#41,type#42,name#43,elevation_ft#44,continent#45,iso_region#46,municipality#47,gps_code#48,iata_code#49,local_code#50,coordinates#51,iso_country#52] Batched: true, DataFilters: [isnotnull(iso_region#46), (iso_region#46 = RU)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/mike/_learn/Spark/newprolab_1/_repos/lectures/src/main/reso..., PartitionFilters: [], PushedFilters: [IsNotNull(iso_region), EqualTo(iso_region,RU)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_region:string,m...

4. Simplify casts - убирает ненужные cast. Например - при попытке каста к тому же самому типу данных

5. Constant folding - сокращает количество констант, используемых в физическом плане

6. Combine filters - объединяет фильтры

Алгоритм работы Dynamic resource allocation:
1) в Spark2 - воркеры начинают подниматься только когда появляются таски, ожидающие обработки (pending), до этого момента - воркеров нет. Можно настроить минимальное количество воркеров, которые всегда будут подняты
2) в Spark3 - сначала RDD/DF запрашивает у ярна ресурсы -> ярн выдает ресурсы ->
датафрейм начинает обсчитыватсья




__6
58-45 => 1-04-50
1-57-00 => 2-05-05


Обзор источников данных:
Spark - это платформа для ОБРАБОТКИ распределенных данных. Она не отвечает за хранение данных и не завязана на какую-либо БД или формат хранения, что позволяет разработать коннектор для работы с любым источником. Часть распространенных источников доступна "из коробки", часть - в виде сторонних библиотек

На текущий момент Spark DF API позволяет работать (читать/записывать) с большим набором источников:
1. текстовые файлы:
  1.1 json
  1.2 text
  1.3 csv
2. бинарные файлы:
  2.1 orc
  2.2 parquet
  2.3 delta
3. базы данных:
  3.1 Elastic
  3.2 Cassandra
  3.3 JDBC
  3.4 Redis
  3.5 Mongo
4. брокеры:
  4.1 Kafka

Для текстовых файлов поддерживаются различные кодеки сжатия (например - lzo/snappy/gzip) Поддерживается чтение как из локальной файловой системы так и из распределенных - HDFS/S3

Чтобы добавить поддержку источника в проект, необходимо:
1. найти нужный пакет на https://mvnrepository.com
  1.1 выбрать актуальную версию для Scala_<version>
  1.2 скачать jar или скопировать команду для нужной системы сборки

2. добавить зависимость в проект:
sbt - в libraryDependencies в файле build.sbt:
libraryDependencies += "org.elasticsearch" %% "elasticsearch-spark-30" % "8.13.4"

Добавить зависимость в приложение можно одним из способов:

3.1 добавить зависимость в spark-submit:
spark-submit --packages org.elasticsearch:elasticsearch-spark-30_2.13:8.13.4
Автоматическая загрузка джарников в кэш хоста, на котором запускается submit
При запуске распределенного приложения джарники будут перемещены в рабочие
директории драйвера/экзекьюторов и добавлены в classpath
  
3.2 добавить jar в spark-submit:
spark submit --jars /path/to/elasticsearch-spark-30_2.13-8.13.4.jar
драйвер + воркеры
  
3.3 добавить зависимость в spark-defaults.conf:
spark.jars.packages org.elasticsearch:elasticsearch-spark-30_2.13:8.13.4
  
3.4 добавить jar в spark-defaults.conf:
spark.jars /path/to/elasticsearch=spark-30_2.13-8.13.4.jar
  
3.5 в runtime с помощью кода:
spark.sparkContext.addJar - !!! добавляет зависимости только в класспасс экзекьюторов


Конфигурации для источника задаются одним из способов:

4.1 через spark-submit:
spark-submit --conf spark.es.nodes=localhost:9200
  
4.2 в spark-defaults.conf:
spark.es.nodes localhost:9200
  
4.3 в коде через SparkSesson:
spark.conf.set("spark.es.nodes", "localhost:9200")
  
4.4 в коде при чтении:
val df = spark.read.format("elastic").option("es.nodes", "localhost:9200")
val df = spark.read.format("elastic).options(Map("es.nodes" -> "localhost:9200", ...))

4.5 в коде при записи:
df.write.format("elastic").option("es.nodes", "localhost:9200)
df.write.format("elastic").options(Map("es.nodes" -> "localhost:9200", ...))
  
1. поддержка источника всегда добавляется на уровне JVM (даже для pySpark) путем добавления в Java classpath нужного класса
2. добавить поддержку источника можно по-разному, однако в большинстве случаев следует избегать "хардкода"
3. можно разработать свой коннектор на Datasource API (v1/v2)


Текстовые форматы:
Spark позволяет хранить данные в текстовом виде в форматах text/json/csv
1. json - JSON строки (не массив JSON документов, а именно раздельные строки, разделенные \n)
2. csv - плоские данные с разделителем
3. text - просто текстовые строки, вычитываются как DF с единственной колонкой (value: String)

Преимущества:
1. простота интеграции
2. поддержка партицирования и сжатия

Недостатки:
1. отсутствие оптимизаций
2. низкая скорость чтения сжатых данных
3. слабая типизация

Запись данных происходит в директорию, внутри которой будут файлы с данными
Это свойство является общим для всех файловых форматов в Spark
Каждый экзекьютор записывает свою партицию на файловую систему (поэтому указывается именно директория, а не имя файла)

SUCСESS/crc файлы вредны т.к. знимают память нейм ноды
1_000_000 файлов в HDFS ~1 Gb хипа в нэйм ноде
Best practiсe - в HDFS маленькие файлы следует объединять в большие + удалять ненужные файлы (SUCСESS/crc)/отключить их запись

!!! При сжатии данные будут занимать меньше места, но у этого решения есть существенный минус - при чтении каждый сжатый файл текстового формата превращается ровно в 1 партицию в DF
При работе с большими датасетами это означает:
1. если файлов мало и они большие - экзекьюторам может не хватить памяти для их чтения (т.к. один сжатый текстовый файл нельзя разбить на несколько партиций)
2. если текстовых файлов много и они маленькие - получаем увеличенный расход памяти в хипе NameNode

Партицированное хранение данных позволяет использовать partition pruning и быстро фильтровать данные по колонкам партицирования
Колонки, по которым выполнено партицирование - не будут входить в состав сохраненного датафрейма, 
в противном случае было бы дублирование

Файловые форматы не имеют автоматической валидации данных при записи => легко ошибиться и записать данные в другом формате

В момент старта спарк сессии определяется с какой файловой системой будет работать Spark - если выставлена переменная окружения HADOOP_CONF_DIR - спарк будет работать с HDFS

Режимы записи:
Spark позволяет выбирать режим записи с помощью метода mode:
1. !!! Overwrite - перезаписывает всю директорию целиком
При работе с базой часто (зависит от коннектора) превращается в Truncate
2. Append - дописывает новые файлы к текущим
3. Ignore - не выполняет запись (no operation режим)
4. Error/ErrorIfExists - возвращает ошибку, если директория уже существует

Семплирование:
Форматы csv/json позволяет автоматически выводить схему из данных. При этом по умолчанию Spark прочитает все данные и выведет схему. Однако при работе с большим датасетом, чтение всего датасета занимает продолжительное время
Решение - опция samplingRatio

1. при чтении/записи текстовых форматов поддерживаются кодеки сжатия, но это создает дополнительные накладные расходы (cpu) и небезопасно на больших объемах датасетов (OOM)
2. при записи в текстовые форматы Spark не выполняет валидацию схемы и формата
3. при включенном выведении схемы из источника - чтение из текстовых форматов происходит дольше
4. !!! не использовать текстовые форматы в проде

В случае необходимости хранения json - сохранять json в parquet


ORC/Parquet:
В отличие от обычных текстовых форматов, ORC/Parquet изначально спроектированы под распределенные системы хранения и обработки

Свойства форматов ORC/Parquet:
1. колоночныe - в них есть колонки и схема, как в таблицах БД
2. бинарныe - прочитать их обычным текстовым редактором не получится
3. имеют похожие показатели производительности и архитектуру, но Parquet используется чаще

Преимущества:
1. наличие схемы данных в файле
2. компрессия на уровне блоков
3. для каждого блока для каждой колонки вычисляется min/max, что позволяет ускорить чтение - PushedFilters

Недостатки:
1. нельзя дописывать/менять данные в существующих файлах
2. необходимо делать compaction

По аналогии с текстовыми форматами, при записи Spark создает директорию и пишет туда все непустые партиции

Формат записи - snappy.parquet (пример файлового - json.gz)
При использовании компрессии сам parquet файл не помещается в сжатый контейнер - вместо этого компрессии подлежат блоки с данными (row group)
Это полностью снимает ограничение, из-за которого чтение сжатых текстовых файлов происходит в 1 поток в 1 партицию

Schema evolution:
При работе с ORC/Parquet, часто воникает вопрос эволюции схемы - изменения структуры данных относительно первоначальных

Best practice - создавать новую колонку с измененным типом данных (а не изменять тип существующей)

Parquet Tools (для ORC - есть ORC Dump) - для диагностики и решения проблем, связанных с parquet
Может работать с файлами на локальной ФС/HDFS

Parquet Tools позволяет:
1. получить схему файла
2. вывести содержимое файла в консоль
3. объединить несколько файлов в один

Если данные пришли в json, то лучше хранить их в parquet
Размер будет больше (~ +20%), но получаем возможность сжатия без вреда для многопоточости - (?)

1. форматы ORC/Parquet позволяют эффективно работать со структурированными данными
2. производительность ORC/Parquet на порядок выше обычных текстовых файлов
3. данные форматы поддерживают сжатие на блочном уровне, что позволяет избегать проблем с многопоточным чтением
4. форматы поддерживают добавление новых колонок в схему, но не изменение текущих


Elastic - документированная распределенная база данных, ориентированная на сложный
полнотекстовый поиск (построена на поисковом движке Lucene)

Преимущества:
1. удобный графический интерфейс Kibana
2. полнотекстовый поиск по любым колонкам
3. встроенная поддержка timeseries
4. поддержка вложенных структур
5. возможность записи данных с произвольной схемой (как плюс так и минус)
6. возможность перезаписывать данные по ключу документа (отсутствие дубликатов - exactly once consistency)

Недостатки:
1. ассиметричная архитектура:
  1.1 клиентские ноды - отвечают за взаимодействие с пользовательскими запросами
  1.2 дата ноды - отвечают за хранение данных
  1.3 мастер ноды - обеспечивают работу кластера
2. скорость записи ограничена самым медленным узлом
3. большие накладные расходы CPU на индексирование
4. ротация (распределение данных по хостам) шардов не всегда проходит хорошо (перекосы)

Основные сущности Elastic:

1. Index - представляет собой "таблицу с данными", если проводить аналогию с реляционными БД
Данные в elastic обычно хранятся в виде индексов, разбитых на сутки (foo-2024-05-29, foo-2024-05-30 и т.д.)
У каждого документа в индексе есть ключ _id (обеспечивает уникальность) и может быть метка времени, по которой в Kibana строятся визуализации

2. Template - шаблон с параметрами, с которыми создается новый индекс (~схема для записи)
Задает ограничения (на уровне схемы документа) для записываемых документов (по умолчанию - ограничений нет)

localhost:5601 -> MANAGEMENT (всплывающая панель слева) -> Dev Tools

Пример шаблона:
#! Legacy index templates are deprecated in favor of composable templates - (?)
PUT _template/airports
{
  "index_patterns": ["airports-*"],
  "settings": {
    "number_of_shards": 1
  },
  "mappings": {
    "dynamic": true,
    "_source": {
      "enabled": true
    },
    "properties": {
      "ts": {
        "type": "date",
        "format": "strict_date_optional_time||epoch_millis"
      }
    }
  }
}

"index_patterns": ["airports-*"] - для всех индексов, начинающихся с airports-*
"number_of_shards: 1 - все данные буду на одном шарде
"dynamic": true - допустима запись документов с любой схемой
"properties": {
      "ts": {
        "type": "date",
        "format": "strict_date_optional_time||epoch_millis"
      }
    }
- в схеме должно быть поле ts с типом "date" в формате "strict_date_optional_time||epoch_millis"

3. Shard - индексы в elastic делятся на шарды
Это позволяет хранить индекс на нескольких узлах кластера

4. Index Pattern - шаблон, применяемый к индексу на уровне Kibana
Позволяет настраивать форматирование и подсветку полей

Перед тем, как начать писать в elastic с помощью Spark, нам необходимо создать шаблон, иначе индекс будет создан с параметрами по умолчанию и построить красивый pie chart в Kibana не получится
Это можно сделать с помощью Dev Tools в Kibana

MANAGEMENT -> Stack Management -> Kibana -> Data Views (опции для красивого отображения)
-> Create data view
-> Name = airports 
Index pattern = airports-*
Timestamp field = ts
-> Save data view to Kibana

ANALYTICS -> Discover (аналитика)

Основной пайплайн работы с Elastic (и другими БД) - записывать в него данные (а не
читать - т.к. медленно)
Spark -> запись данных в Elastic -> аналитика/визуализация в Kibana

!!! Количество партиций в DF равно количеству шардов индекса

1. Elastic - удобное распределенное хранилище документов, не накладывающее строгих ограничений на схему документов
2. Elastic позволяет делать сложные запросы, включая полнотекстовые
3. при работе с Elastic, Spark часто использует predicate pushdown
4. Spark отлично подходит для того, чтобы писать в Elastic. Однако чтение работает не очень быстро
5. User friendly для аналитиков - Kibana
6. при отказе ноды, кластер будет временно недоступен
7. кластер только внутри одного дата-центра (будет лучше, если узлы будут расположены рядом на высокоскоростных свитчах)


Cassandra - распределенная wide-column база данных

Преимущества:
1. высокая доступность данных (при отказе ноды - кластер остается доступным)
2. возможность построения гео-кластеров
3. высокая скорость записи и чтения
4. скорость ограничена самым быстрым узлом
5. линейная масштабируемость (без downtime)
6. возможность хранить БОЛЬШИЕ объемы данных
7. возможность быстро получать строку по ключу на любом объеме данных

Недостатки:
1. слабая согласованность (eventual - т.е. есть возможность прочитать старые данные)
2. "бедный" SQL (в кассандре называется Cassandra Query Language - CQL)
3. отсутствие транзакций (есть lightweight transaction)

Ведется работа по добавлению обычных транзакций

Cassandra имеет симметричную архитектуру. Каждый узел отвечает за хранение данных, обработку запросов и состояние кластера

Расположение данных на нодах определяется значением хэш-функции (Murmur3) от partition key
Высокая доступность (HA) обеспечивается за счет репликации (запись производится на n-1 следующих узлов в кольце кластера, n - фактор репликации)

Ключ может быть составным и состоять из:
1. Partition key - колонки по которым рассчитывается Murmur3 hash
2. Clustering key - колонки, определяющие сортировку внутри каждой партиции
Partition keys + Clustering keys = Composite key

От выбранных partition/clustering keys (определяются на этапе создания таблицы)
зависит возможность постороения различных типов запросов

Кластер кассандры называется кольцом
token(n1) = t1		range(t1, t2] => {n2, n3, n4}
token(n2) = t2		range(t2, t3] => {n3, n4, n5}
token(n3) = t3		range(t3, t4] => {n4, n5, n6}
...

Есть вторичные индексы по обычным колонкам (не являющимися ключами)

Сущности cassandra:
1. Keyspace - аналог database - логическое объединение таблиц
на уровне keyspace устанавливается фактор репликации
2. Table - таблицы (как в обычной бд)

При записи производится Upsert = Update (для существующих ключей) + Insert

Скорость чтения ОЧЕНЬ СИЛЬНО зависит от структуры таблицы и запроса. Если сделать запрос по колонке, являющейся partition key, то будет применена оптимизация predicate pushdown (в Spark3 - Cassandra Filters: [["ident" = ?, 22WV]]) и запрос отработает очень быстро (т.к. зная ключ - cassandra быстро установит на каком хосте находятся данные)

Если сделать запрос по колонке не являющейся partition key, то predicate pushdown также может отработать, а может и нет - в этом случае cassandra будет искать эти данные на всем кластере (full table scan)
(в Spark3 - Cassandra Filters: [])

Скорость записи в Cassandra/Kafka - почти никогда не является узким местом

1. Cassandra - одна из немногих БД, которая способна эффективно хранить большие объемы данных
2. в Cassandra структура таблицы формируется, исходя из запросов, которые будут выполняться, а не наоборот
3. в Cassandra даннные обычно хранятся в денормализованном виде (~по таблице на каждый запрос)
4. Spark отлично подходит для записи данных в Cassandra, для чтения нужна настройка под конкретный профиль нагрузки


PostgreSQL - классическая СУБД

При записи производится Insert

По умолчанию чтение производится в 1 партицию
Скорректировать это поведение можно, используя параметры partitionColumn/lowerBound/upperBound/numPartitions

Spark выводит схему JDBC-источника с помощью запроса:
SELECT * FROM table_name WHERE 1 = 0 LIMIT 1;
!!! Но гарантий оптимизации этого запроса в конкретной базе - нет, может быть FULL TABLE SCAN

1. Spark позволяет работать с PostgeSQL через JDBC коннектор
2. при использовании JDBC настройка партицирования задается вручную


Основные компоненты для написания кастомного коннектора:
1. определить механизм чтения данных в партицию - RDD[InternalRow]
2. определить механизм вывода схемы - StructType




__7
9-50
1-09-50 => 1-15-00
1-58-55 => 2-05-10
2-53-05


Особенности систем потоковой обработки данных:
1. работают с непрерывным потоком данных
2. нужно хранить состояние стрима
3. результат обработки быстро появляется в целевой системе
4. должны проектироваться с учетом требований к высокой доступности
5. важная скорость обработки данных и время задержки (лаг)

Примеры систем потоковой обработки данных:
1. Обработка платежей по картам:
  1.1 нельзя терять платежи
  1.2 нельзя дублировать платежи
  1.3 простой сервиса недопустим - нужно High Availability
  1.4 максимальное время задержки ~1с
  1.5 небольшой поток событий
  1.6 OLTP
  
2. Обработка логов безопасности:
  2.1 потеря единичных событий допустима
  2.2 дублирование единичных событий допустимо
  2.3 простой сервиса допустим
  2.4 максимальное время задержки ~1ч
  2.5 большой поток событий
  2.6 OLAP
  
Виды стриминговых систем:
1. Real-time streaming (Flink):
  1.1 низкие задержки на обработку
  1.2 низкая пропускная способность
  1.3 подходят для критически важных систем (высокая доступность)
  1.4 пособытийная обработка
  1.5 OLTP
  1.6 Exactly once consistency - нет потери данных и нет дубликатов
  
2. Micro batch streaming (Spark):
  2.1 высокие задержки
  2.2 высокая пропускная способность
  2.3 не подходит для business-critical систем
  2.4 обработка батчами
  2.5 OLAP
  2.6 At least once consistency - во время сбоев могут возникать дубликаты

Real-time streaming на Spark строить нельзя т.к. он не удовлетворяет требованиям
Real-time streaming - C++/Java/Scala + OracleDB
  
1. Spark Structured Streaming является micro-batch системой
2. при работе с большими данными обычно пропускная способность важнее, чем время задержки


Rate streaming:
Самый простой способ создать стрим - использовать rate источник (простой счетчик)
Созданный DF является streaming, о чем нам говорит метод создания readStream и атрибут isStreaming
Rate хорошо подходит для тестирования приложений, когда нет возможности подключиться к потоку реальных данных

Cинк = сток/потребитель/получатель = DataStreamWriter

В отличие от обычных датафреймов, у стриминговых нет таких методов как show/collect/take, также недоступен Dataset API - для просмотра их содержимого, нужно использовать console синк и создать StreamingQuery
!!! Процессинг начинается только после вызова метода start и не блокирует основной поток приложения
Trigger позоляет настроить частоту чтения/обработки новых данных

Стриминговый датафрейм как и обычный имеет партиции (можно выполнять repartition/groupBy/join)
В заданный момент времени обрабатывается конкретный микробатч (микробатч под капотом = RDD[InternalRow]), состоящий из определенного числа партиций

Автоматический перезапуск стрима:

1. Стрим упал, но приложение не упало - например ошибка трансформации/OOM на экзекьюторе

Перезапуск возможен из приложения после анализа ошибки
sink.start() - неблокирующая операция - стрим работает асинхронно

while (true) {
  Try(sq.awaitTermination(15000)) }
  ...
}

awaitTermination(15000) блокирует основной поток на 15с - далее стрим или продолжит работу или упадет с ошибкой
Т.е. через каждые 15 секунд работы мы можем запустить свою логику - например rest api для управления стримом снаружи (стартовать/перезапускать)
Более простое решение - маркер файл на HDFS - если появляется (например stop_file) - остановить стрим

2. Стрим упал и приложение упало - например зафейлились таски -> зафейлился стрейдж -> несколько перезапусков стейджа (если все еще фейлится) -> фейл приложения

Можно перезапустить через oozie/airflow/cron
Если платформа стримовая - стрим надо мониторить
!!! Стрим должен работать в единственном экземпляре - zookeeper/consul/маркер файл на HDFS

Socket source - лучше не использовать

ForeachBatch Sink - позволяет применить произвольную функцию к микробатчу (фактически к обычному DF)

Чекпоинты - метаинформация, позволяющая корректно перезапускать стримы с места остановки

!!! Spark не читает файлы:
1. tmp
2. начинающиеся с "_"
3. начинающиеся с "."

Best practice стриминговой записи на HDFS:
1. repartition - уменьшаем количество файлов, которые будут записаны
2. compaction

Варианты записи стримингового потока более крупными частями (уменьшение количества файлов):
1. trigger(Trigger.ProcessingTime("10 seconds")) - увеличить время триггера
2. createParquetSink(streamDf.repartition(1), "s1.parquet") - репартицирование в меньшее количество партиций

Производительность стрима завязана на количество партиций и количество экзекьюторов (как и для батчевых задач). Часто ограничена пропускной способностью приемника (elastic/db)

Оценка производительности стрима:
1. lastProgress/recentProgress:
  1.1 inputRowsPerSecond - прочитано строк/c
  1.2 processedRowsPerSecond - обработано строк/c - более честный параметр

Параллельно внутри одного Spark приложения можно запускать несколько стримов (т.к. они работают асинхронно) - антипаттерн по мнению Андрея Титова

1. rate - самый простой способ создать стрим для тестирования приложений
2. стрим начинает работу после вызова метода start() и не блокирует основной поток программы


File Streaming:
Spark позволяет запустить стрим, который будет "слушать" директорию (локальная ФС/HDFS) и читать из нее новые файлы. При этом за раз будет прочитано количество файлов, установленное в параметре maxFilesPerTrigger
В этом кроется одна из основных проблем данного источника. Поскольку стрим, сконфигурированный под чтение небольших файлов, может "упасть", если в директорию начнут попадать файлы большого объема

В чекпоинте хранится информация об обработанных файлах

Можно стримить как плоскую так и вложенную (/*) структуру директорий
Можно стримить из parquet/orc/json/text/csv - важно, чтобы был единый формат данных для всего стрима

!!! В директорию могут попать любые данные и DF должен иметь фиксированную схему =>
файловый стриминг нельзя создать без указания схемы

Недостатки File streaming:
1. входной поток можно ограничить только максимальным количеством файлов, попадающих в батч
2. гранулярность обработки - батч
Если стрим упадет в середине обработки батча, то при перезапуске все файлы этого батча будут обрабатываться снова

File streaming не стоит использовать на проде


Kafka streaming:
Apache Kafka - самая распространенная в мире система, на основе которой строятся приложения для поточной обработки данных

Предназначена для записи/чтения больших объемов данных
Снижает связанность системы - приложение продюсер никак не связано с приложением консьюмером
=> можно останавливать/обновлять/... продюсер/консьюмер приложения независимо

Преимущества:
1. высокая пропускная способность
2. высокая доступность за счет распределенной архитектуры (выход из строя ноды не приводит к остановке работы) и репликации
3. у каждого сообщения есть свой номер, который называется offset, что позволяет гранулярно сохранять состояние стрима
Это позволяет повторно обработать уже обработанные сообщения. Сообщение при чтении не удаляется

Технически Kafka - не брокер очередей (нет понятия очереди), а distributed commit log (распределенный журнал коммитов)

Архитектура системы:
Topic - это логическая таблица в Kafka. Мы пишем данные в топик и читаем данные из топика 
Топик, как правило, распределен по нескольким брокерам (= узел кластера Kafka = нода) для обеспечения высокой доступности и скорости работы с данными

Partition - это физические блоки, из которых состоят топики. Партиция представляет собой неделимый блок, который хранится на одном из узлов. Топик может иметь произвольное количество партиций
Чем больше партиций - тем выше параллелизм при чтении/записи, однако большое число партиций в топике может привести к замедлению работы всей системы

Replica - каждая партиция может иметь несколько реплик (т.е. репликация в Kafka работает на уровне партиций). Внешние приложения всегда работают (читают/пушат) с основной репликой. Остальные реплики являются дочерними и не используются во внешнем IO - только синхронизируют данные из основной реплики. Если узел, на котором расположена основная реплика, падает, то одна из дочерних реплик становится основной и работа с данными продолжается

Message - это данные, которые мы пишем и читаем в Kafka - представляют собой кортеж (key, value)
Ключ используется не всегда - часто имеет значение null
Сериализация/десериализация данных всегда происходят на уровне клиентов Kafka
!!! Kafka ничего о типах данных не знает и хранит ключи/значения в виде массива байт

Ключ нужен для партиционирования данных внутри топика

Offset(Long - 2^64) - это порядковый номер сообщения в партиции. При записи сообщения (сообщение всегда пишется в одну из партиций топика) Kafka помещает его в топик с номером n+1, где n - номер последнего сообщения в этом топике. Оффсеты нумеруются независимо для каждой партиции

Адрес сообщения в Kakfa - {имя топика} : {id партиции} : {оффсет}

Producer - это приложение, которое пишет в топик. Producer'ов может быть много
Параллельная запись достигается за счет того, что каждое новое сообщение попадает в случайную (round-robin partitioning) партицию топика (если не указан key)

Consumer - это приложение, читающее данные из топика. Consumer'ов может быть много, в этом случае они называются consumer group (группе назначается идентификатор - id)
Параллельное чтение достигается за счет распределения партиций топика между consumer'ами в рамках одной группы. Каждый consumer читает данные из "своих" партиций и ничего про другие не знает
Если consumer падает - его партиции переходят другим consumer'ам (происходит перевыбор партиций - на это время чтение данных прекращается)

!!! Если консюмеров больше чем партиций - лишние консюмеры будут использовать ресурсы впустую

Сommit - называют сохранение информации о факте обработки сообщения с определенным оффсетом
Поскольку оффсеты для каждой партиции топика свои, то и информация о последнем обработанном оффсете хранится по каждой партиции отдельно
Обычные приложения пишут коммиты в специальный топик Kafka __consumer_offsets

!!! Spark не использует стандартный механизм управления оффсетами Kafka
Управление оффсетами в Spark происходит с помощью чекпоинтов, которые сохраняются на ФС (например HDFS)
Spark сохраняет обработанные оффсеты по каждому батчу

Retention - поскольку кластер Kafka не может хранить данные вечно, в конфигурации задаются пороговые значения по объему и/или времени хранения для каждого топика, при превышении которого данные удаляются
Например - если у топика A установлен retention по времени 1 месяц, то данные будут хранится в системе не менее одного месяца (и затем будут удалены одной из внутренних подсистем)

Best practice - для удобного последующего поиска оффсета с нужным таймстэмпом - после обработки микробатча записывать в БД offset:timestamp
Т.к. обходить топики и каждый раз искать оффсеты с нужными таймстэмпами - долго


Работа с Kafka с помощью Static DF:

Spark позволяет работать с Kafka как с обычной БД
Для записи в Kafka необходимо подготовить датафрейм с 2-мя колонками:
1. value: String - данные, которые мы хотим записать
2. topic: String - топик, куда писать каждую строку датафрейма
String - тк легко конвертируется в Array[Byte]

Kafka использует Zookeeper для синхронизации узлов (ранее в зукипере также хранились оффсеты)

!!! Особенности чтение из Kafka:
1. по умолчанию читается все содержимое топика. Поскольку обычно в нем много данных, эта операция может создать большую нагрузку на кластер Kafka и Spark приложение
2. колонки value/key имеют тип binary, который необходимо десериализовать

Чтобы прочитать только определенную часть топика, необходимо задать минимальный и максимальный оффсеты для чтения с помощью параметров startingOffsets/endingOffsets
По умолчанию параметр startingOffsets имеет значение earliest, а endingOffsets - latest
Поэтому если не указывать эти параметры - Spark прочитает содержимое всего топика

Чтобы получить данные из binary - их необходимо десериализовать
Формат данных может быть произвольным (если json - достаточно cast(StringType))

!!! Spark управляет Сonsumer group'ами сам, собственный механизм Kafka - не используется

При использовании статических датафреймов с Kafka - если нужно хранить последний прочитанный оффсет - делать вручную


Работа с Kafka с помощью Streaming DF:

При создании streamingDf необходимо помнить:
1. startingOffsets - по умолчанию имеет значение latest
2. endingOffsets использовать нельзя (тк стрим бесконечен)
3. !!! количество сообщений за батч можно (и нужно) ограничить параметром maxOffsetsPerTrigger - по умолчанию он не задан - первый батч будет вычитывать данные всего топика

Гранулярность - 1 offset топика Kafka

Если maxOffsetsPerTrigger ~= lastProgress.processedRowsPerSecond стрим лагает, т.е. чтение не успевает за источником

Чекпоинты лучше писать на HDFS (чекпоинты на локальной файловой системе могут быть повреждены при отключении питания)

В чекпоинте лежат последние оффсеты по каждому топику
src/main/resources/l_7/chk/s3.kafka/offsets
Если нужно перечитать данные из Kafka (с произвольного места) - нужно удалить файлы чекпоинта и в параметре startingOffsets указать нужное значение




__8
4-40 => 6-10
44-25 => 50-40
1-43-55 => 1-50-00


Stateful streaming - это вид поточной обработки данных, при которой при обработке батча с данными используются данные из предыдущих батчей

Все операции с использованием select/filter/withColumn (кроме операций с плавающими окнами) являются stateless

Stateless streaming:
1. стрим не выполняет операций, требующих работы с данными из разных батчей
2. после обработки батча, стрим "забывает" про него
3. высокая пропускная способность
4. небольшое количество файлов и общий объем чекпоинта
5. возможность вносить существенные правки в код стрима без пересоздания чекпоинта

Statefull streaming - если при обработке стрима используются такие методы как join/groupBy/dropDuplicates или функции c плавающими окнами:
1. в стриме должна быть колонка с временной меткой, но основе которой можно определить watermark
2. statefull стрим работает медленнее, чем stateless
3. в чекпоинте будет МНОГО файлов (хранение агрегатов)
4. при внесении изменений в код стрима с большой вероятностью придется пересоздавать чекпоинт


Удаление дубликатов:
Spark позволяет удалять дубликаты данных в стриме. Это можно сделать 2-мя способами:
1. Без использования watermark:
  1.1 хэш каждого элемента будет сохраняться в чекпоинте - директория state
  1.2 в стриме полностью исключаются дубликаты
  1.3 если число уникальных значений большое - со временем начнется деградация стрима (из-за 1.1)
  
2. С использованием watermark:
  2.1 хэш старых событий удаляется из чекпоинта
  2.2 появление дубликатов возможно, если они приходят с задержкой N > watermark
  2.3 стрим не деградирует со временем
  2.4 колонка, по которой делается watermark, должна быть включена в dropDuplicates

Watermark - пороговое значение, рассчитывающееся на основании колонки с таймстемпом данных
!!! В датафрейме обязательно наличие колонки с временной меткой

Алгоритм работы watermark:
1. Maximum Observable Timestamp (MOT) - максимальный таймстэмп в предыдущем батче
MOT обновляется с каждым батчем
2. Threshold - порог (например 5 минут)
3. Event timestamp - время события

!!! Если eventTimestamp < MOT - threshold - событие игнорируется

dropDuplicates - делает неявное репартицирование

При использовании watermark фильтрация дублей происходит в 2 этапа:
1. отбрасываются старые события, не подходящие под Event timestamp < MOT - threshold
2. удаляются дубликаты по полям watermark + указанные поля

Нахождение оптимальной watermark - баланс между условиями бизнеса и нагрузкой на стрим (чем больше watermark - тем больше хэшей хранить)

Если необходимы абсолютно уникальные значения (а не только в рамках watermark) - нужно проверять совпадение полей не через statefull streaming, а через udf которая будет обращаться к очень быстрой БД (например redis) в каждом батче


Агрегаты:
!!! При построении агрегатов на стриме важно задать правильный ouputMode, который может иметь 3 значения:

1. Complete - каждый батч будет дописываться в бесконечную таблицу
Агрегаты будут рассчитываться на каждом батче по ВСЕЙ таблице
Если кардинальность ключей высокая - со временем стрим будет деградировать

!!! В Complete нельзя использовать watermark => Complete можно использовать для statefull, только при низкой кардинальности ключей (~ до 1_000_000 значений)

Если писать в паркет - на каждом батче будет записываться новый файл, если использовать cassandra/mongo - на каждом батче ВСЕ данные будут перезаписываться => большая нагрузка на БД

2. Update - будем получать только дельты, т.е. строки, которые поменялись в агрегате после обработки текущего батча
!!! Записывать в Update режиме в cassandra/mongo - дешевле чем в Complete, т.к. перезаписываться будут только изменения

Можно использовать watermark - т.е. режим подходит для высококардинальных ключей:
  2.1 watermark + агрегат на указанных колонках
  2.2 агрегаты на плавающих окна

3. Append - ПО УМОЛЧАНИЮ. После обработки батча Spark "забывает" про него
В Append режиме в синк будут записаны только ЗАВЕРШЕННЫЕ окна с данными в момент window_right_bound + watermark_value
!!! Без watermark - не работает

!!! Удобно, когда нужно ЕДИНОЖДЫ записать ФИНАЛЬНЫЙ результат (т.к. он больше не изменится) - например в реляционную БД (снижаем нагрузку на БД)


Агрегаты на плавающих окнах:
Плавающее (sliding) окно позволяет сгруппировать события в окна определенного размера (по времени) При этом, поскольку каждое событие может находиться одновременно в нескольких окнах, то общий размер агрегата существенно увеличивается

Окно задается при создании агрегата с помощью функции window внутри groupBy
В параметрах указывается длина окна и расстояние между точкой начала текущего окна и следующего окна

1. Watermarks поддерживаются в режимах Append/Update
2. Плавающее окно имеет два параметра - размер окна и сдвиг текущего окна относительно следующего


Соединения:
Spark позволяет делать:
1. Stream-Static join (работают как джоины на обычных датаферймах):
  1.1 Inner
  1.2 Left Outer
  1.3 Left Anti
2. Stream-Stream join:
  2.1 Inner
  2.2 Left Outer
  2.3 Right Outer
  
Stream-Static join - может использоваться для:

1. Left Outer - Обогащения стрима фактами
!!! Best practice - соединять с небольшой таблицей в формате parquet
Не стоит джойнить с большими таблицами из БД (даже из быстрых типа Cassandra) т.к. для джоина надо прочитать оба датафрейма ЦЕЛИКОМ (механика джоина) - т.е. на каждом батче будет вычитываться ВСЯ огромная таблица из БД
В Spark3 - появился оператор directCassandraJoin, который работает быстрее BroadcastHashJoin, но для highload также не подойдет
Подобную логику можно реализовать с помощью udf с получением данных из БД с помощью connection pool

2. Inner - фильтрация по white list

3. Left Anti - фильтрация по black list
  
!!! Можно использовать watermark для удаления старых данных из стрима

Stream-Stream join:
Для соединения двух стримов необходимо одно из двух:

1. Добавить к условию соединения равенство двух окон - джоин событий по интервалу времени
(т.к. таймстэмп одного и того же события - почти всегда будет отличаться)
 
2. Добавить к условию соединения сравнение двух временных меток
Можно использовать округление таймстэмпов

Стейт стрима хранит все данные по незакрытым окнам

StreamingSymmetricHashJoin - технически работает как HashJoin (был удален из Spark) - строит карту хэш-мап для партиций

Stream-Static Left/Inner/Left Anti join - работают в Append режиме без обязательного указания watermark

Watermark нужен только для stream-stream join (?)

Получение отброшенных (по watermark) событий:
В lastProgess -> раздел eventTime - есть значения watermark + метрики времени - на основании этих данных потенциально можно получить отброшенные события - создать еще 1 стрим и на основании batchId/eventTime получать отброшенные события (но это двойное чтение источника + возможность рассинхронизации стримов)
Нормального решения - нет (только постпроцессинг)




__9
45-30 => 54-00
2-02-55 => 2-09-00


Foreach Batch Sink:
Проблемы обычных синков:

!!! Два стрима, даже если они созданы на основе одного датафрейма, не могут использовать общий чекпоинт =>
1. стримы являются независимыми и работают асинхронно
2. чтение данных из источника происходит у каждого стрима НЕЗАВИСИМО и АСИНХРОННО (серьезное увеличение нагрузки на источник)
3. один стрим может начать отставать (лагать) от другого стрима

Обычные синки не позволяют использовать кастомную логику работы стрима (например в зависимости от поступающих данных)

!!! В Spark 2.4 появился foreachBatch синк - это синк, позволяющий применить произвольную функцию к каждому батчу в стриме, работая с ним, как со СТАТИЧЕСКИМ датафреймом

1. Стандартные синки позволяют создавать простые пайплайны, в реальной жизни чаще используется foreachBatch

2. foreachBatch - каждый микробатч, передающийся в foreachBatch будет представлен как статический:
  2.1 можно использовать любое API внутри функции
  2.2 можно использовать cache/persist
  2.3 можно выполнять запись в несколько разных мест
  2.4 поддерживает режимы append/update/complete
  2.5 можно использовать batchId
  

FAIR Scheduler:
До этого момента мы всегда выполняли action'ы над датафреймами один за другим, т.е. последовательно

Данный подход имеет  существенный недостаток - низкая утилизация ресурсов, предоставленных Spark приложению, поскольку каждое действие блокирует основной поток на драйвере и не позволяет выполняться следующим, даже если у приложения еще есть свободные ресурсы

Вышеписанная проблема не является критичной для обычных ETL приложений, однако в стримах, где задержка обработки данных является одним из ключевых параметров, простой выделенных ресурсов недопустим 

Используя метод par - можно запустить вычисления на драйвере одновременно для всех датафреймов, однако планировщие Spark приложения работает в режиме FIFO (все доступные ядра забирает первая джоба, если ядер больше, чем нужно первой джобе - они отдаются второй и т.д.), поэтому на экзекьюторах партиции разных датарфеймов будут все еще обрабатываться ПОСЛЕДОВАТЕЛЬНО - плохо, например при записи в несколько источников (пиковая нагрузка на каждый источник)

Если переключить режим планировщика в FAIR, то все действия (!!! в рамках стейджа) буду выполняться ПАРАЛЛЕЛЬНО
FIFO - плохой вариант при записи в несколько мест (например в 3 базы)
FAIR - даст меньшую нагрузку на базы + лучше параллелизм

!!! FAIR режим можно использовать и для работы со static DF

Для этого необходимо устновить две опции в spark-defaults.conf:
1. spark.scheduler.mode FAIR
2. spark.scheduler.allocation.file /path/to/fairscheduler.xml
Если запуск производится в cluster mode - нужно указать данный файл через --file

Файл fairscheduler.xml должен содержать:
<?xml version="1.0"?>
<allocations>
  <pool name="default">
    <schedulingMode>FAIR</schedulingMode>
    <weight>1</weight>
  </pool>
</allocations>

1. Spark позволяет запускать действия параллельно
2. Для параллельного запуска действий должны быть выполнены следующие условия:
  2.1 планировщие Spark должен быть переведен в FAIR режим
  2.2 пул default должен быть переведен в FAIR режим
  2.3 действия должны запускаться на драйвере параллельно (можно использовать любое API, поддерживающее multithreading)

Для выполнения shuffle - драйвер должен знать где какие ключи лежат, драйвер составляет карту ключей на экзекьюторах

!!! В одну и ту же папку нельзя одновременно записывать паркеты из разных датафреймов

Future - позволяет более гранулярно управлять действиями, чем parallel collection




__10
1-40
1-23-00 => 1-27-15


В BigDataTools можно работать с удаленным кластером Spark - как (?)

Тестирование можно производить на dev-кластере в режиме YARN (важно правильно параметризировать тесты)
Установить sbt на dev-кластер и запускться из него
Конфиги задавать через переменные окружения/отдельный yaml-файл

InternalRow - строка с данными в датафрейме (не включает схему)

методы trait Source
1. def getOffset: Option[Offset] - перед началом работы cтрима проверяет есть ли новые данные в источнике
2. def getBatch(start: Option[Offset], end: Offset): DataFrame - вызывается, если есть новые данные 
3. def commit - вызывается после каждого батча (getBatch)
Можно реализовать свою логику коммита - нужна базе, а не спарку
4. def stop - можно имплементировать кастомную логику действий после остановки стрима

Из JDBC можно создать стриминговый источник

!!! Минорная (2.X.14) версия скалы в джарнике ДОЛЖНА соответствовать версии скалы в дистрибутиве спарка

./spark-shell ->
util.Properties.versionString - версия скалы
sc.version - версия спарка




__11
1-03-00 => 1-09-25
1-50-30


Настройка spark-submit окружения:
Дистрибутив Spark содержит в себе различные библиотеки, примеры конфигурационных файлов и набор утилит

Любое Spark приложение запускается одной из утилит:
1. spark-shell - запуск интерактивного Scala REPL с поддержкой Spark
2. pyspark - запуск интерактивного python шела с поддержкой Spark
3. spark-submit - запуск Spark приложений, собранных в виде jar/py файлов с зависимостями
spark-submit - обертка над специальным классом, запускающим приложение

spark-submit/spark-shell/pyspark - находятся в каталоге bin
При запуске Spark задач, под капотом используются именно эти утилиты
Достаточно, чтобы они были установлены только на хосте, с которых происходит запуск приложения
На обычных узлах кластера они не нужны

beeline - cmd утилита для подключения к thrift/Hive для выполнение sql запросов
spark-sql - позволяет выполнять нативные sql запросы (запускает спарк джобу)

./spark-submit --help
->
Usage: spark-submit [options] <app jar | python file | R file> [app arguments]
Usage: spark-submit --kill [submission ID] --master [spark://...]
Usage: spark-submit --status [submission ID] --master [spark://...]
Usage: spark-submit run-example [options] example-class [example args]

Options:
  --master MASTER_URL (выбор менеджера ресурсов кластера):
  	1. spark://host:port - standalone - используется спарковский кластер менеджер
  	Демон на каждой ноде
  	1 общий application master на все спарк приложения
  	Хороший вариант для бэкэнда (не нужно пускать пользователей, не нужна ad-hoc аналитика)
  	2. mesos://host:port
  	3. yarn - на каждое ярн приложение создается отдельный application master
  	Приоритеты указываются в разрезе очередей
  	Resource overcommitment - 
  	Preemption - механизм, позволяющий убивать контейнеры менее приоритетных задач, для
  	запуска контейнеров более приоритетных
  	4. k8s://https://host:port
  	5. local
  	
  	Default: local[*]
                              
  --deploy-mode DEPLOY_MODE
  	Whether to launch the driver program locally ("client") or
        on one of the worker machines inside the cluster ("cluster")
  	
  	1. сlient - драйвер будет запущен на локальной машине ->
  	все логи будут приходить в локальную консоль
  	2. сluster - драйвер будет запущен в кластере
  	Удобно для серверного приложения, без взаимодействия с пользователем
  	Приложение будет работать после отключения ssh-сессии
  	
  	Default: client
 
  --class CLASS_NAME
  	 Your application's main class (for Java / Scala apps)
  	
  --name NAME
  	A name of your application
  	
  --jars JARS 
  	Comma-separated list of jars to include on the driver and executor classpaths
  	
  --packages
  	Comma-separated list of maven coordinates of jars to include on the driver and executor 
  	classpaths. Will search the local maven repo, then maven central and any additional
  	remote repositories given by --repositories.
        The format for the coordinates should be groupId:artifactId:version
        
  	Место хранения джарников на файловой системе - <user>/ivy2
  	
  --exclude-packages
  	Comma-separated list of groupId:artifactId, to exclude while
        resolving the dependencies provided in --packages to avoid dependency conflicts
  	
  --repositories
  	Comma-separated list of additional remote repositories to
        search for the maven coordinates given with --packages
                              
  --py-files PY_FILES
  	Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps
  	
  --files FILES
  	Comma-separated list of files to be placed in the working directory of each executor.
  	File paths of these files
  	
  	Можно передать файл - например log4j2.properties/ssl-сертификат/...
  	
  --archives ARCHIVES
  	Comma-separated list of archives to be extracted into the working directory of each executor

  --conf, -c PROP=VALUE
  	Arbitrary Spark configuration property
  	
  --properties-file FILE
  	Path to a file from which to load extra properties.
  	If not specified, this will look for conf/spark-defaults.conf

  --driver-memory MEM
  	Memory for driver (e.g. 1000M, 2G)
  	
  	Default: 1024M
  	
  --driver-java-options 
  	Extra Java options to pass to the driver
  	
  	Например - выбор gc
  	
  --driver-library-path
  	Extra library path entries to pass to the driver
  	
  --driver-class-path
  	Extra class path entries to pass to the driver.
  	Note that jars added with --jars are automatically included in the classpath
  	
  	Указываем дополнительные класспасы для драйвера

  --executor-memory MEM
  	Memory per executor (e.g. 1000M, 2G)
  	
  	Default: 1G

  --proxy-user NAME
  	User to impersonate when submitting the application.
        This argument does not work with --principal / --keytab

  --help, -h
  	Show this help message and exit
  	
  --verbose, -v
  	Print additional debug output
  	
  --version,
  	Print the version of current Spark
  	
 Spark Connect only:
  --remote CONNECT_URL
  	URL to connect to the server for Spark Connect, e.g.,
        sc://host:port. --master and --deploy-mode cannot be set together with this option.
        This option is experimental, and might change between minor releases

 Cluster deploy mode only:
  --driver-cores NUM 
  	Number of cores used by the driver, only in cluster mode
  	
        Default: 1

 Spark standalone or Mesos with cluster deploy mode only:
  --supervise
  	If given, restarts the driver on failure

 Spark standalone, Mesos or K8s with cluster deploy mode only:
  --kill SUBMISSION_ID
  	If given, kills the driver specified
  	
  --status SUBMISSION_ID
  	If given, requests the status of the driver specified

 Spark standalone and Mesos only:
  --total-executor-cores NUM
  	Total cores for all executors

 Spark standalone, YARN and Kubernetes only:
  --executor-cores NUM
  	Number of cores used by each executor.
        Default: 1 in YARN and K8S modes, or all available cores on the worker in standalone mode

 Spark on YARN and Kubernetes only:
  --num-executors NUM
  	Number of executors to launch
  	If dynamic allocation is enabled, the initial number of executors will be at least NUM
  	
  	Default: 2
                              
  --principal PRINCIPAL
  	Principal to be used to login to KDC
  	
  	Для Кербероса
  	
  --keytab KEYTAB
  	The full path to the file that contains the keytab for the principal specified above

 Spark on YARN only:
  --queue QUEUE_NAME
  	The YARN queue to submit to
  	
  	Default: "default"

В каталоге python/lib находятся зависимости для python - при настройке среды разработки не нужно ставить pyspark в свой venv. Правильнее - добавить эти библиотеки из дистрибутива - избегаем возможных конфликтов версий в будущем

В каталоге jars находятся все необходимые библиотеки, включая классы драйвера/воркеров, библиотеки для работы с Hadoop/встроенными источниками данных (например - parquet)

В каталоге conf находятся шаблоны конфигурационных файлов:
1. spark-defaults.conf - основной конфигурациооный файл
Общие для всех пользователей настройки нужно указать в spark-defaults.conf,
отличающиеся - через параметры спарк-сабмита/properties-файл
!!! Properties файл полностью оверрайдит дефолтный
2. log4j2.properties - конфигурация логирования, можно задать необходимый уровень логирования для различных компонентов Spark (включая собственные 
3. spark-env.sh - скрипт, в котором устанавливаются переменные окружения (например - HADOOP_CONF_DIR/YARN_CONF_DIR)
!!! Переменные окружения имеют больший вес, чем настройки в spark-defaults.conf
4. metrics.properties
Можно отправлять метрики (например - Java метрики/стриминговые метрики/...) в Графит (например)
Встроенные метрики - не очень информативны
!!! Библиотека dropWizard позволяет имплементить кастомные метрики

Приоритет указания параметров конфигурации:
1. параметры, указанные в spark-submit
2. переменные окружения
3. spark-defaults.conf

1. Наличие Hadoop не является необходимым условием для работы Spark
2. spark-submit позволяет запустить Spark приложение как локально, так и на кластере


Работа со Spark UI:
Каждое Spark приложение по умолчанию поднимает UI, который позволяет изучить состояние задачи и провести диагностику производительности
!!! Данные, представленные в UI, также можно получить через REST API

count запускает проекцию в 0 колонок (т.к. важно только количество строк) - поэтому это быстрая операция

Количество экзекьюторов - Spark UI -> Executors

Если Yarn status = Accepted, но контейнер с драйвером не поднимается => не хватает ресурсов
Если Yarn status = Running (т.е. драйвер запустился), но контейнеры с воркерами не поднимаются => не хватает ресурсов 

Кэшировать паркет не имеет особого смысла т.к. это быстрый источник
Медленные источники - базы данных/статический DF из кафки/заархивированные текстовые форматы

Запись в паркет промежуточных результатов - страховка при ошибках в долгих джобах




__12

19-50
55-30 => 1-02-30

Рекомендуется spark.locality.wait = 0s

InternalRow является упорядоченной коллекцией


.withColumn("domain", callUDF("parse_url", 'domain, lit("HOST")))
https://spark.apache.org/docs/latest/api/sql/#parse_url

