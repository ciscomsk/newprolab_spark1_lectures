https://lk-scala4.newprolab.com/
mikhail.kuznetzov
T20jKm1v

ssh -i newprolab.pem name.surname@spark-master-6.newprolab.com
ssh spark-de-master

ssh mikhail.kuznetzov@spark-node-1.newprolab.com -D localhost:1080 -N -i newprolab.pem



Настройка среды Intellij IDEA для Scala-проектов
https://github.com/newprolab/spark-scala-2/blob/main/idea.md

Настройка Intellij IDEA для программы "Apache Spark для задач дата инжиниринга"
https://www.youtube.com/watch?v=Tf73EZbYMKM

Подключение к мастер ноде кластера из консоли по ssh, используя приватный ключ
https://github.com/newprolab/spark-scala-2/blob/main/ssh.md

Подключение к YARN UI
https://github.com/newprolab/spark-scala-2/blob/main/proxy.md

Пример сборки fat-jar
https://github.com/MrSandmanRUS/ScalaAssemblyPluginExample


Запустить Jupyter Notebook
jupyter notebook

Запустить scala-приложение
scala <scala.jar>

Поиск файла
locate spark-shell




__1

Jupyter Notebook
https://www.8host.com/blog/ustanovka-jupyter-notebook-dlya-python-3-v-ubuntu-20-04-i-podklyuchenie-po-ssh-tunnelyu/

Алгоритм Raft
https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_Raft

Apache Kafka — скоро без ZooKeeper
https://habr.com/ru/company/southbridge/blog/552688/

Java FileSystem API
https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html

MapReduce WordCount example
https://github.com/naver/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java

Configuring Mountable HDFS (FUSE - Filesystem in Userspace)
https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_ig_hdfs_mountable.html

ORC Files
https://cwiki.apache.org/confluence/display/hive/languagemanual+orc




__2 (???)

Jupyter ноутбук занятия
https://github.com/Gorini4/jupyter_scala_docker

Ammonite
http://ammonite.io/

Scastie
https://scastie.scala-lang.org/

ScalaFiddle
!Не работает https://scalafiddle.io/

IntelliJ IDEA
https://www.jetbrains.com/idea/download/

Альтернативы IntelliJ IDEA
https://docs.scala-lang.org/getting-started/index.html#open-hello-world-project

Scalding
https://github.com/twitter/scalding

What are all the uses of an underscore in Scala?
https://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala

Scala Exercises
https://www.scala-exercises.org/

A Comprehensive Guide to For-Comprehension in Scala
https://www.baeldung.com/scala/for-comprehension

Collections performance characteristics
https://docs.scala-lang.org/overviews/collections-2.13/performance-characteristics.html

Benchmarking Scala Collections
https://www.lihaoyi.com/post/BenchmarkingScalaCollections.html

What are the differences between final class and sealed class in Scala?
https://stackoverflow.com/questions/32199989/what-are-the-differences-between-final-class-and-sealed-class-in-scala

Classes and Objects
https://www.scala-lang.org/files/archive/spec/2.13/05-classes-and-objects.html

Tour of Scala
https://docs.scala-lang.org/tour/tour-of-scala.html

Scala access modifiers and qualifiers in detail
http://www.jesperdj.com/2016/01/08/scala-access-modifiers-and-qualifiers-in-detail/

Alvin Alexander blog
https://alvinalexander.com/

Scala Cookbook
https://www.amazon.com/Scala-Cookbook-Object-Oriented-Functional-Programming-dp-1492051543/dp/1492051543

Json4s
https://github.com/json4s/json4s

Circe
https://circe.github.io/circe/

Typesafe config
https://github.com/lightbend/config

PureConfig
https://github.com/pureconfig/pureconfig

Scalatest
https://www.scalatest.org/

Sbt 
https://www.scala-sbt.org/
sbt_shell: compile/run

???
чат


__3 (???)

Scala 2.13.9 API
https://www.scala-lang.org/api/current/

RDD Programming Guide
https://spark.apache.org/docs/latest/rdd-programming-guide.html

RDD
https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html

PairRDDFunctions
https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html

Using Transient Lazy Val's To Avoid Spark Serialisation Issues
https://nathankleyn.com/2017/12/29/using-transient-and-lazy-vals-to-avoid-spark-serialisation-issues/

???
l_3.RDD => почему counterAccum не всегда равен rdd4.count()
2 вопроса в коде


__4

Scala QUASIQUOTES
https://docs.scala-lang.org/overviews/quasiquotes/intro.html

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html

Функции для expr()
https://spark.apache.org/docs/latest/api/sql/index.html

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Column.html
https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/TypedColumn.html

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameNaFunctions.html

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/RelationalGroupedDataset.html

https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/expressions/WindowSpec.html

Beyond traditional join with Apache Spark
https://www.linkedin.com/pulse/beyond-traditional-join-apache-spark-kirill-pavlov/


__5 (???/work)

Janino
https://janino-compiler.github.io/janino/

Sort-merge join in Spark SQL
https://www.waitingforcode.com/apache-spark-sql/sort-merge-join-spark-sql/read

Optimizing Apache Spark SQL Joins: Spark Summit East talk by Vida Ha
https://www.youtube.com/watch?v=fp53QhSfQcI

https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/index.html

work
Реализовать toUpperCase (перевести название всех полей в верхний регистр) в test org.apache.spark.sql.SchemaSpec

???
l_5.DataFrame_5 => не работает def printCodeGen




__6 (???)

https://spark.apache.org/docs/latest/sql-data-sources-csv.html
https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option
https://spark.apache.org/docs/latest/sql-data-sources-json.html
https://spark.apache.org/docs/latest/sql-data-sources-text.html


https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
https://spark.apache.org/docs/latest/sql-data-sources-orc.html
https://docs.delta.io/latest/quick-start.html

https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html#spark-sql
https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md
https://github.com/datastax/spark-cassandra-connector#documentation
https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html
https://github.com/RedisLabs/spark-redis/blob/master/doc/dataframe.md
https://docs.mongodb.com/spark-connector/master/scala-api/

https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html

Опции для конкретного источника можно посмотреть в методах.
https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html


Фёдор Лаврентьев, Moscow Spark #5: Как класть Parquet.
https://www.youtube.com/watch?v=VHsvr10b63c&t=512s

https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option

??? Где взять parquet tools со сборкой на Java.
https://pypi.org/project/parquet-tools/


How to Do a Clean Restart of a Docker Instance.
https://docs.tibco.com/pub/mash-local/4.1.1/doc/html/docker/GUID-844A3791-ACED-449E-A0BF-D7B7FD54C6FF.html


https://www.elastic.co/
https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-dev-mode
https://www.elastic.co/guide/en/kibana/current/docker.html#_running_kibana_on_docker_for_development
!!! Работает только с Spark 2.4.8 - https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20
https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html


https://cassandra.apache.org/_/index.html
https://cassandra.apache.org/doc/latest/architecture/dynamo.html#dataset-partitioning-consistent-hashing
https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector


https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html
https://mvnrepository.com/artifact/org.postgresql/postgresql


__7 (???)

https://kafka.apache.org/

https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources
https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks

https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html

https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10

???
Проверить, что для разных агрегаций одного датафреймы строятся разные графы (т.е. нужен cache).


__8 (???)

???
Посмотреть лабу 4-b.


__10 (???)

Шаблоны конфигов.
https://github.com/apache/spark/tree/branch-3.2/conf

???
чат


__11

https://spark.apache.org/downloads.html

Список всех конфигураций спарка.
https://spark.apache.org/docs/latest/configuration.html

Мониторинг спарк приложений (метрики/REST/...).
https://spark.apache.org/docs/latest/monitoring.html

YARN overcommitment
https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/OpportunisticContainers.html

YARN preemption
https://datacadamia.com/db/hadoop/yarn/preemption


__12

spark.locality.wait
https://spark.apache.org/docs/latest/configuration.html#scheduling




__1

5-00 => 40-35 -
1-48-55 => 2-01-25
2-12-15 - hdfs cli команды

Архитектуры хранилищ:
1. DWH - schema on write => схема задается при записи
2. Data lake - schema on read => схема задается при чтении

Чаще всего DWH строится поверх Data lake

Модели обработки данных:
1. ETL == Extract Transform Load
2. ELT == Extract Load Transform

Под словом Hadoop могут подразумеваться:
1. Core сервисы Hadoop (HDFS/YARN/MapReduce)
2. Вся экосистема сервисов Hadoop
3. Конкретный вычислительный кластер под управлением Hadoop

Предпосылки появления Hadoop:
1. Потребность в распределенных хранилищах
2. Масштабирование вычислений
3. Управление ресурсами

Core сервисы Hadoop:
1. HDFS == Hadoop Distributed File System - распределенное файловое хранилище
2. YARN == Yet Another Resource Negotiator - менеджер ресурсов
3. MapReduce - фреймворк для распределенных вычислений

Cервисы на основе/использующиеся вместе c Hadoop:
1. Data ingestion (поглощение) tools - инструменты импорта/экспорта данных на hdfs
  1.1 Flume - экспорт данных в hdfs
  Альтернативы - NiFi/StreamSets
  1.2 Sqoop - импорт/экспорт данных RDBMS <=> HDFS
  Альтернативы - Debezium/Kafka Connect
  1.3 Kafka - очередь с огромной пропускной способностью
  Альтернативы - Pulsar/Nutch/RabbitMQ/Kinesis(Amazon)/PubSub(Google)
2. On HDFS - HBase(wide column database)
3. On YARN - Spark/Flink/Storm(стриминг)/Lucene Solr(обработка текстовой информации)
4. On MapReduce - Hive/Pig/Mahout
5. Zookeeper (HDFS, Kafka (отказывается), YARN, ClickHouse) - metadata storage
6. Cassandra

Zookeeper - это централизованная служба для поддержки информации о конфигурации, именования, обеспечения распределенной синхронизации и представления групповых служб
Zookeeper - это каталог, хранящий метаинформацию - данные о мастер ноде/воркерах/тасках/... Информация хранится в z-нодах

Линеаризуемость - события должны передаваться/обрабатываться в порядке их поступления
Линеаризуемость - одна из гарантий консистентности данных

Zookeeper - распределенный сервис (есть leader node) - это гарантирует отказоустойчивость и балансировку/масштабирование нагрузки

CAP - Consistency Availability Partition tolerance
Система может соответствовать только 2-м критериям CAP

1. CA - нераспределенная система
2. AP - распределенная система, главная цель - Availability
Пример - горячие кэши - лучше иметь неконсистентые кэши в разных частях системы, чем не иметь их вообще (большая нагрузка на БД)
3. CP - распределенная система, главная цель - Consistency
В случае возникновения неконсистентного состояния в части системы (split brain) - эта часть отключается (ноды)

Zookeper - CP система
etcd/consul - системы близкие по назначению с Zookeeper

HDFS (Hadoop Distributed File System) - файловая система, предназначенная для хранения файлов большого размера, поблочно распределенных между узлами вычислительного кластера

Элементы архитектуры HDFS:
1. Name node - хранит метаданные (маппинги файл => блоки) - единая точка отказа
2. Data node - хранит блоки данных
3. Secondary Name node - нэйм нода передает сюда fsimage и логи, отбратно возвращается fsimage c вмерженными из логов изменениями 

Клиент выполняет чтение/запись напрямую на дата нодах (данные не перегоняются через нэйм ноду - у нее запрашиваются только метаданные) => большая пропускная способность

Файл - это только запись в метаданных на name node
Содержимое файла хранится в нескольких блоках одинакового размера на data node

Фактор репликации по умолчанию == 3
HDFS умеет различать не только разные ноды, но и разные стойки (rack) - топология конфигурируется => возможно обеспечение отказоустойчивости на уровне дата центра

Поблочная запись обеспечивает параллельное чтение (не ограничиваемся пропускной способностью 1-й ноды) + равномерная утилизация свободного места
Перекосы блоков на нодах исправляются балансировщиком

Чтение из HDFS (термины Java API):
1. Клиент (jvm) создает объект Distributed File System
2. Get block location from name node (конвертация имени файла в список блоков на нэйм нодах)
3. Клиент создает FS Data Input Stream
4. Чтение из дата нод
5. close() on client

Запись на HDFS (термины Java API):
1. Клиент (jvm) создает объект Distributed File System
2. Create on name node (проверка - возможно файл с таким именем уже существует)
3. Клиент создает FS Data Output Stream
4. Запись данных - write packet + ack packet
5. Репликация (фоновая) записанных блоков между дата нодами - write packet + ack packet
6. close() on client
7. complete on name node

High availability (HA) ZK cluster - используется 2 нэйм ноды (active + stand by)
В z-ноде хранится id active нэйм ноды
4. ZKFC (Zookeeper Failover Controller) - подключены к active/standby нэйм нодам
ZKFC определяют какая нэйм нода является active в данный момент
5. Journal node
Active нэйм нода посылает на кластер с журнальными нодами (shared storage of journal nodes) изменения. Standby нэйм нода вычитывает изменения из кластера журнальных нод
6. Name node federation

Если используется HA HDFS кластер - клиенту нужно подложить конфиги (hdfs-site.xml/core-site.xml из /etc/hadoop/conf) => active нэйм нода будет определяться автоматически

Особенности HDFS:
1. Не любит мелкие файлы - нагрузка на ram в нэйм ноде (1 файл - 1 запись в нэйм ноде)
Нужно репартицировать ~под размер блока
2. Не поддерживает редактирование файла - файл удаляется и записывается снова
3. Работает на дешевом железе

HDFS API:
1. libhdfs - C API
2. FileSystem - нативное Java API
3. WebHDFS/HttpFS - REST API
4. HDFS CLI - hdfs dfs -<command>

Python HDFS API:
1. hdfs
2. hdfs3
3. snakebite

При удалении из hdfs - файлы сначала помещаются в .Trash - затем автоматически (через ~24 часа) удаляются

HDFS - это object storage

Типы хранилищ:
1. Object storage - работают не на уровне фс, а по своим протоколам/апи
Object storage менее производительны, но более универсальны/масштабируемы чем block storage
2. Block storage - работают на уровне фс - ~большой диск
3. File storage

Распределенные хранилища:
1. HDFS (S3 - Amazon/GCS - Google) - объектное хранилище
2. Minio - объектное хранилище, хорошо подходит для хранения мелких файлов
Проще конфигурируется + хорошо разворачивается в kubernetes
3. Ceph - более универсален и производителен, чем HDFS, но гораздо сложнее
4. Gluster
5. Seaweedfs - для хранения очень маленьких файлов
6. Apache Ozone - развитие HDFS


Системы контейнеризации - отвечают за управление ресурсами + изоляцию работающих процессов:
1. Kubernetes
2. Mesos
3. YARN

YARN (Yet Another Resource Negotiator) - модуль отвечающий за управление ресурсами кластера (v2 - ram/cpu, v3 +gpu) и планирование заданий

Resource manager - управляет ресурсами кластера
Node manager - управляет ресурсами конкретной ноды (физической машины)

Мехагтзм запуска приложения в YARN:
1. Client =(job submission - .jar)=> Resource manager
2. Resource manager => Node manager => разворачивается контейнер с Application master
3. Application master =(resource request)=> Resource manager (success) => Node manager => разворачиваются контейнеры для обработки задачи (воркеры)

Обмен сообщениями:
1. Воркер =(result status)=> Application master
2. Node manager =(node status)=> Resource manager

Контейнер - отдельный jvm процесс в системе
Если контейнер превысил допустимый порог потребления по памяти - YARN убивает контейнер
Если Application master падает - все дочерние контейнеры убиваются

В YARN используется двухфазный планировщик ресурсов:
1. Клиент при запуске приложения указывает необходимые ресурсы
2. Приложение может запрашивать дополнительные/отдавать освободившиеся ресурсы (пример - dynamic allocation в Spark)

YARN использует механизм очередей (приоритеты + выделяемые ресурсы (min/max) + логика планирования (fifo/fair)):
1. Отдельная очередь для долгих тяжелых задач
2. Отдельная очередь для мелких ad-hoc запросов
3. Отдельная очередь для стриминговых задач
4. Отдельная очередь для обучения моделей
5. Отдельная очередь на каждый отдел и т.д.

:8088 - YARN web UI
yarn -kill <yarn_id> - убить ярн процесс из консоли
spark submit ... --queue <queue_name> - указание очереди для ярна


MapReduce - подход к организации распределенных вычислений (статья MapReduce: Simplified Data Processing on Large Clusters)

Главная идея MapReduce - вычисления можно представить как последовательность операций:
1. Map - трансформации
2. Shuffle - перераспределение данных по ключам
3. Reduce - агрегации

MapReduce стабильнее, но гораздо медленнее Spark


HIVE - SQL движок, для выполнения запросов к слабоструктурированным данным (запрос преобразуется в MapReduce джобу)
Для запросов используется язык HiveQL
HIVE имеет свой metastore для хранения метаинформации (структура таблиц)

В HIVE нельзя делать атомарные (where) Update/Delete - т.к. данные - файлы на HDFS
Для OLAP нагрузки характерен принцип append only

Выполнение запроса в хайве:
1. Hive sql => CLI/JDBC/ODBC/WebUI => HiveServer
=>
Hive:
2.1 HiveServer
2.2 MetastoreDB - хранит маппинги имя таблицы => путь в фс
2.3 Compiler
2.4 Optimizer
2.5 Executor 
=>
3. MapReduce job on YARN Hadoop

HIVE может использовать движки Spark/Tez (работают быстрее), вместо MapReduce

Партицирование - разбиение данных на уровне файловой системы  
show create table <table_name> - посмотреть какой командой была создана таблица (в т.ч. информацию по партицированию)

Виды форматов файлов хранения:
1. Колоночные
ORC/Parquet - последовательное чтение (гораздо быстрее чем случайное чтение с диска) нужных колонок + отличное сжатие (однородные данные сжимаются лучше) + индексы
Нельзя записывать построчно => не подходят для потоковой записи
Лучший формат для аналитики
2. Строковые
AVRO - бинарный формат (компактное хранение данных) + схема не дублируется в каждом сообщении
AVRO - лучший выбор для потоковой записи

В хайве можно описать собственный формат сериализации/десериализации через SerDe

Internal table - удаляются и метаданные и данные
External table - удаляются только метаданные, данные остаются

Если во внешней партицированной таблице HIVE после записи новых данных их не видно (запросы sql):
msck repair table <table_name> - обновление метастора (перечитает партиции)
+ invalidate metadata если используется Impala

Варианты написания запросов в HIVE:
1. Beeline/Hive CLI
2. HUE (Human User Experience) - web UI
3. Библиотеки - JDBC/pyhive/...

В HIVE можно строить ACID (Atomicity Consistency Isolation Durability) таблицы в формате ORC - производительность очень низкая (можно использовать для небольших таблиц)
В случае необходимости ACID для больших таблиц (например - таблица фактов) лучше использовать Delta/Iceberg/Hudi - записываются файлы с дельтами относительно предыдущего состояния (версионируемость) + логи транзакций + фоновый компакшн




__2

6-00
53-00 => 59-30
1-49-25 => 1-59-00
3-01-30 => 3-08-30


Scala:
1. Объектно-ориентированный
2. Функциональный
3. Статически-типизированный
4. Работает поверх JVM
5. Можно использовать Java-библиотеки

Scala фреймворки:
1. Анализ данных и ETL - Spark
2. Потоковая обработка - Flink
3. Распределенные приложения - Akka
4. Параллельные и асинхронные вычисления - Monix/Cats effect/ZIO

Консоль:
1. Scala
2. Ammonite

Онлайн компиляторы:
1. Scastie
2. ScalaFiddle

IDE:
1. IntelliJ IDEA + Scala plugin
2. VsCode/...

Типы данных:
1. Char/Int/Double/...
2. Tuple<N>
3. Immutable collections - List/Set/Map/...
4. Mutable collections
5. Option/Try
6. Unit ()

Иерархия коллекций:
1. Traversable
2. Iterable
  2.1 Seq
    2.1.1 IndexedSeq - Array/Range/String/Vector/StringBuilder/ArrayBuffer - индексы
    2.1.2 LinearSeq - List/Queue/Stack/Stream/LinkedList/MutableList - связи между элементами
    2.1.3 Buffer - ArrayBuffer/ListBuffer
  2.2 Set
  2.3 Map

null - нет значения
None - подтип Option
Nil - пустой список
Nothing - подтип всех типов

dataset.map(x => myCustomTransform(x)) - это выражение будет одинаковым для:
1. Scala коллекций
2. Параллельных Scala коллекций
3. Spark Datasets
4. Flink DataStreams
5. Потоков сообщений Akka

Иерархия типов:
1. Any
  1.1 AnyVal
    1.1.1 Double
    1.1.2 Float
    1.1.3 Long
    1.1.4 Int
    1.1.5 Short
    1.1.6 Byte
    1.1.7 Unit
    1.1.8 Boolean
    1.1.9 Char
      1.1.1 Nothing
  1.2 AnyRef (== java.lang.Object)
    1.2.1 List
    1.2.2 Option
    1.2.3 Other classes ...
      1.2.1.1 Null
        1.2.1.1.1 Nothing
        
Элементы ООП в Scala:
1. Class
2. Object
3. Trait

Case class (СС):
1. Удобны для моделирования неизменяемых данных
2. Сравниваются по структуре, а не по ссылке
3. Имеют метод copy - для быстрого копирования неизменяемых объектов
4. Не нужно ключевое слово new при создании
5. companion object из коробки => apply/unapply/toString/equals/hashCode
7. extends Serializable из коробки
8. Immutable

СС широко используются для сериализации и описания данных:
1. Библиотеки JSON поддерживают автоматическое кодирование/декодирование CC
2. Spark использует CC для описания типизированных наборов данных - Datasets
3. Akka Typed

Object:
1. Singleton - класс только с одним экземпляром
2. Используется вместо статических методов в классе
3. Scala App - это объект

Companion object (CO):
1. CO - объект с тем же именем, что и класс
2. Определен в том же файле
3. CO может обращаться к методам/полям, являющимися приватными в соответствующем
классе/трейте

СО применяется для:
1. Размещения конструкторов
2. Размещения статических методов
3. ...

Trait:
1. Похож на Mixin или Interface в Java
2. Основная задача - определяет контракт на реализацию конкретной функциональности
3. Нет конструктора
4. Можно подмешивать неограниченное количество трейтов

Абстрактный класс:
1. Способ описать общие свойства и позволить потомкам реализовать их с помощью собственной логики
2. Можно наследовать только 1 класс

Generic class - класс с параметризированным типом. Это позволяет создавать классы с общей функциональностью для разных типов

Модификаторы доступа:
Модификатор	Класс	Компаньон	Подкласс	Пакет	Мир
no modifier	  +	    +		    +		  +	 +
protected	  +	    +		    +		  -	 -
private	  +	    +		    -		  -	 -

Анонимный класс - позволяет реализовать/расширить класс на "лету"

Final классы не могут быть расширены
Sealed классы могу быть расширены только в том же файле

Implicit class (IC) - использутся для неявного расширения функционала других классов

Неявные преобразования применяются в двух случаях:
1. Если выражение e имеет тип S, а S не соответствует выражениям предполагаемого типа T.
В этом случает ищется преобразование c, которое применимо к e, и тип результата которого соответствует T
2. При вызове e.m, где экземпляр e имеет тип S, но у типа S отсутствует метод m.
В этом случае ищется преобразование c, которое применимо к e, и результат которого содержит член с именем m

Полезные библиотеки:
1. Парсинг Json:
  1.1 Json4s
  1.2 Circe
2. Работа с конфигурацией:
  2.1 Typesafe config
  2.2 PureConfig

Idea => Settings => Build, Execution, Deployment => Build Tools => sbt
sbt shell => use for:
1. project reload
2. builds

Для разбора результата генерации scala-кода можно использовать:
1. scalac -Xprint:all <file>.scala - покажет все шаги работы компилятора
2. javap code.class - покажет дизассемблированный код результат




__3

59-55 => 1-04-45
1-40-10 => 1-46-15
3-01-20


Spark - фреймворк для создания распределенных приложений для обработки данных
Spark является эволюцией концепции Hadoop MapReduce

Область применения:
1. Распределенная обработка больших данных
2. Построение ETL пайплайнов
3. Работа со структурированными данными (SQL)
4. Разработка стриминговых приложений

Архитектура:
1. Driver (== Master) - всегда 1 - может быть запущен в кластере (cluster)/вне кластера (client)
  1.1 Предоставляет api через SparkSession/SparkContext
  1.2 Выполняет код - скомпилированный jar
  1.3 Управляет выполнением задачи
  1.4 Не занимается обработкой данных
2. Workers (== Executors/Slaves)
  2.1 Обрабатывает данные
  2.2 Каждый worker работает со своим сегментом данных - partition
  2.3 Код на воркере не выполняется напрямую - через api код с драйвера преобразуется и применяется
  2.4 Получает задачи от driver
3. Cluster Manager (Kubernetes/YARN/Mesos)
  3.1 Отвечает за аллокацию контейнеров, выполняющих код драйвера/воркеров на кластере
  3.2 Квотирует ресурсы между пользователями
  3.3 Контролирует состояние контейнеров

Driver/Worker'ы - обычные jvm приложения (могут быть многопоточными)
SparkContext - отвечает за загрузку конфигурации/запуск воркеров/...

1 Task == 1 Partition == 1 OS thread (Cpu core)

Компоненты Spark DAG (Directed Acyclic Graph):
1. Job
2. Stage - разные стэйжди разделяются shuffl'ом (пе
3. Task - процесс обработки 1-ой партиции 1-м ядром

RDD (Resilient Distributed Dataset) - типизированная неизменяемая неупорядоченная партицированная коллекция данных, распределенную по узлам кластера
RDD - самая базовая/низкоуровневая структура в Sparkдоступная разработчику

RDD API - низкоуровневое API, которое позволяет применять любые функции к распределенным данным
При использовании RDD API обработка всех исключительных ситуаций лежит на плечах разработчика

RDD можно создать из:
1. Локальная коллекция на драйвере
2. Файл - в локальной/распределенной файловой системе (например HDFS)
3. Базы данных

Виды операций с RDD:
1. Transformation (трансформации) - map/filter/...
2. Action (действия) - reduce/collect/take/count/foreach/...

Transformations:
1. Всегда превращают исходный RDD в новый RDD
2. Всегда являются ленивыми (lazy) - создают граф вычислений, но не запускают их
Гораздо эффективнее оптимизировать/выполнить полный граф 1 раз, чем выполнять операции по одной
3. Иногда (часто) неявно требуют перемещения данных между воркерами - shuffle

Actions:
1. Выполняют действия над RDD
2. Запускают граф вычислений

RDD - неизменяемы => всегда создается новый RDD
Элементами RDD могут быть коллекции/кейс классы/кортежи/...

Обычный воркер - 1/2 ядра + 8 Gb ram
Большое количество памяти на воркере => большая нагрузка от GC

Партиция - это ленивый Iterator, но при записи его необходимо материализовать в памяти целиком
Обработка партиций производится параллельно, данные внутри партиии обрабатываются последовательно
Количество партиций/количество элементов в партиции в RDD/DF зависит от имплементации конкретного источника (определяет разработчик коннектора)

PairRDD - расширенный класс функций, доступных для RDD, где элементы - это кортеж из 2-х элементов (key, value)

Join позволяет соединить два PairRDD по ключу
PairRDD поддерживают join(inner), leftOuterJoin, fullOuterJoin

При работе с RDD API - функции сериализуются на драйвере => передаются на каждый воркер => десериализуются и выполняются

Option[T] - монада, позволяющая работать с отсутствующими данными, избегая исключительных ситуаций и обработки null. Одним из ее преимуществ является то, что ее можно рассматривать как коллекцию, что позволяет применить к RDD[Option[T]] метод flatMap, который вернет RDD[T], убрав все None из датасета

Результат shuffle - конкретные файлы на файловой системе воркеров

В local mode - нет воркеров, все происходит внутри драйвера => нет сериализации/десериализации




__4

42-25 => 47-55
1-52-45 => 1-58-30
2-51-35


Сравнение RDD API и DataFrame API:
1. Типы данных:
RDD: низкоуровневая распределенная коллекция данных ЛЮБОГО типа
DF: таблица со схемой, состоящей из колонок ОПРЕДЕЛЕННЫХ типов, описанных в org.apache.spark.sql.types

2. Обработка данных:
RDD: сериализация (java serializer) функций на драйвере => десериализация и применение на воркере
DF: кодогенерация SQL (Thungsten) => java код => компилятор janino => применение скомпилированного кода к данным

3. Функции и алгоритмы:
RDD: нет ограничений
DF: ограничен SQL операторами, функциями org.apache.spark.sql.functions и пользовательскими функциями udf

4. Источники данных:
RDD: каждый источник имеет свое API
DF: единое API для всех источников - интерфейс DataSource v1/v2

5. Производительность:
RDD: напрямую зависит от качества кода
DF: встроенные механизмы оптимизации SQL запроса

6. Потоковая обработка данных:
RDD: устаревший DStream
DF: активно развивающийся Structured Streaming

На текущий момент RDD является низкоуровневым API, которое постепенно уходит "под капот" Apache Spark. DF API представляет собой библиотеку для обработки данных с использованием SQL примитивов

Ресурсы для создания dataframe:
1. Локальные коллекции
2. Файлы
3. Базы данных

1. Методы filter/select принимают в качестве аргументов колонки org.apache.spark.sql.Column
Это может быть либо ссылка на существующую колонку, либо функция из org.apache spark.sql.functions
2. Любые трансформации возвращают новый DF, не меняя существующий
3. Тип org.apache.spark.sql.Column играет важную роль в DF API - на его основе создаются ссылки на существующие колонки, а также функции, принимающие org.apache.spark.sql.Column и возвращающие org.apache.spark.sql.Column
По этой причине обычное сравнение == не будет работать в DF API, т.к. filter принимает org.apache.spark.sql.Column, а не Boolean.
4. Класс DataFrame начиная со Spark 2 представляет собой org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] поэтому его описание следует искать в org.apache.spark.sql.Dataset


Очистка данных:
DF API содержит класс функций "not available", описанный в пакете org.apache.spark.sql.DataFrameNaFunctions

1. na.drop - удаление строк с null
2. na.fill - заполнение значений null
3. na.replace - замена значений

Общая концепция - избавиться от null:
1. В Spark - null + 1 == null
2. udf и ряд встроенных функций уязвимы к null

distinct - получает уникальные записи для всех колонок датафрейма
dropDuplicates - позволяет указать колонки, по которым будут получены уникальные записи


Агрегации:
agg позволяет использовать любые aggregate functions из пакета org.apache.spark.sql.functions

collect_list/collect_set/pivot - "тяжелые агрегаты"

Используя методы struct/to_json мы можем превратить произвольный набор колонок в JSON строку - часто используются перед отправкой данных в Kafka

DF API позволяет строить большое количество  агрегатов. При этом необходимо помнить, что операции groupBy/cube/rollup возвращают org.apache.spark.sql.RelationalGroupedDataset, к которому затем необходимо применить одну из функций агрегации - count/sum/agg/...

При вычислении агрегатов необходимо помнить, что эта операция требует перемешивания
данных между воркерами, что, в случае перекошенных данных, может привести к OOM на воркере


Кэширование:
По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может
негативно сказаться на производительности приложения

cache/persist/repartition - позволяют не вычислять граф несколько раз
Механизм кэширования работает одинаково для RDD/DF

cache/persist - сохраняют состояние графа после первого действия, и следующие обращаются к нему 
persist - позволяет выбрать, куда сохранить данные
cache - использует значение по умолчанию - в текущей версии Spark это StorageLevel.MEMORY_ONLY

Важно помнить, что данный кэш не предназначен для обмена между разными Spark приложениями - он является внутренним для конкретного приложения. После окончания работы с данными необходимо выполнить unpersist для очистки памяти

Кэш в Spark - LRU, т.е. перезаписывает наименее используемые данные

Использование cache/persist позволяет существенно сократить время обработки данных, однако следует помнить об увеличении потребляемой памяти на воркерах

StorageLevel.MEMORY_ONLY - при заполнении кэша, новые данные будут затирать старые
StorageLevel.MEMORY_AND_DISK - при заполнении кэша в памяти, новые данные будут вытеснять старые на диск. Кэш на диске не перетирается новыми данными => место на диске может закончится


Репартиционирование:
RDD и DF представляют собой классы, описывающие распределенные коллекции данных
Эти коллекции разбиты на крупные части, которые называются партициями

В графе вычислений Spark DAG (Direct Acyclic Graph) есть 3 основные компонента:
1. Job - представляет собой весь граф целиком, от момента создания DF до применения action к нему Job состоит из одной или более stage
2. Stage - возникает когда необходимо сделать shuffle данных
Stage состоит из task
3. Task - базовая операция над данными
Одновременно Spark выполняет N task, которые обрабатывают N партиций, где N - это суммарное число доступных ядер на всех воркерах

Как следствие важно обеспечить:
1. Достаточное количество партиций для распределения нагрузки по всем воркерам
Количество партиций нужно делать кратно больше числа ядер (x2, x3, x4)
Эмпирический коэффициент == 3 => 100 ядер => 300 партиций
2. Равномерное распределение данных между партициями

spark.conf.set("spark.sql.shuffle.partitions", <partition_number>) - подбирается индивидуально исходя из ресурсов кластера (по умолчанию == 200)

!!! Передача данных между стейджами (шафл) выполняется через файлы =>
=> репартиционирование считается дорогой операцией

Алгоритм выполнения репартицирования:
1. Внутри каждой партиции выполняется сортировка данных по ключам
2. На драйвере формируется карта расположения ключей по партициям на воркерах
3. Каждый воркер открывает фетч-реквесты к другим воркерам, получает нужные данные
и сохраняет их на свою файловую систему =>
=> Результат репартиционирования - файлы на локальных дисках воркеров
4. Возникает 2-й стэйжд (например повторный HashAggragate) - в рамках которого выполняется чтение данных c файловой системы. Далее работа производится в памяти


Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников


DataSkew при агрегациях:
Поскольку при вычислении агрегата происходит неявный HashPartitioning по ключам агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к постороению агрегата

Один из вариантов - соление ключей. Оно позволяет существенно снизить объем данных в перекошенных партициях. При key salting агрегировать нужно дважды (во второй раз нужно суммировать агрегаты), но при этом распределение данных по партициям будет более равномерным, что позволит избежать OOM на воркерах

Не любую задачу можно решить key salting - например distinct count
Проблему слишком больших партиций можно попробовать решить разбиением исходного датасета на более мелкие

1. Партицирование - важный аспект распределенных вычислений, от которого напрямую зависит стабильность и скорость вычислений
2. В Spark всегда работает правило 1 TASK == 1 CORE == 1 PARTITION
3. Репартицирование и соление данных позволяет решить проблему перекоса данных и вычислений
4. Репартицирование использует дисковую и сетевую подсистемы (при использовании стандартного шафл-сервиса) - обмен данными происходит по сети, а результат записывается на диск, что может стать узким место при выполении репартицирования


Встроенные функции:
Помимо базовых SQL операторов в Spark существует большой набор встроенных функций:
1. API методы из spark.sql.functions
2. SQL built-in functions

1. Spark обладает широким набором функций для работы с колонками разных типов:
стороки/числа/.../словари/массивы/структуры
2. Встроенные функции принимают колонки org.apache.spark.sql.Column и возвращают org.apache.spark.sql.Column в большинстве случаев
3. Встроенные функции можно и нужно использовать вместе - на вход встроенных функций могут подаваться результаты других встроенной функции, т.к. все они возвращают sql.Column


Пользовательские функции:
UDF (user defined function) - могут принимать до 16 аргументов

null в Spark превращается в null внутри UDF

Пользовательская функция может возвращать:
1. Простой тип: String/Long/Float/Boolean/... (Scala type)
2. Массив - любые коллекции, наследующие Seq[T] - List/Vector/...
3. Карту - Map[A, B]
3. Case class
4. Option[T]

1. Пользовательские функции позволяют реализовать произвольнй алгоритм и использовать его в DF API
2. Пользовательские функции работают медленнее встроенных, поскольку при использовании встроенных функций Spark использует ряд оптимизаций, например векторизацию вычислений на уровне CPU


Соединения:
Позволяют соединять два DF в один по заданным условиям

Виды join'ов по типу условия:
1. equ-join - соединение по равенству одного или более ключей
2. non-equ join - соединение по условию, отличному от равенства одного или более ключей

Виды join'ов по методу соединения:
1. Inner
2. Left Outer
3. Right Outer
3. Full Outer
4. Cartesian (Cross)
5. Left Anti
6. Left Semi

Условия соединения можно задать с помощью:
1. Seq(String)
2. sql.Column

В условии джоина может присутствовать массив Seq(String) - это синтаксический сахар, позволяющий не перименовывать колонки датасетов, а просто указать, что соединение будет производиться по колонкам с именами, входящими в массив. 

В более общем виде условия (например - col("left_a") === col("right_a")) допускается использование встроенных функций, пользовательских функций и операторов сравнения. Однако следует помнить, что мы выполняем джоин двух распределенных датасетов и если условие соединения будет составлено плохо - Spark выполнит cross join производительность которого крайне мала

Под капотом при джоине может производится репартицирование


Оконные функции:
Позволяют выполнять функции над "окнами" данных

Окно создается из класса org.apache.spark.sql.experssion.Window с указанием:
1) полей, определяющих границы окон
2) полей, определяющих порядок сорировки внутри окна

Окно определяется списком колонок и сортировок

Применяя окна, можно использовать такие полезные функции из org.apache.spark.sql.functions как lag/lead, а также эффективно работать с time-series данными

!!! Применение оконных функций приводит к shuffle
!!! row_number().over(пустое окно) == Exchange single partition (антипаттерн) => OOM

Use-cases оконных функций:
1. Агрегатные функции
2. Вычисления над значениями одной колонки из разных строк (c orderBy т.к. это гарантирует сортировку)
3. Нумерация строк в рамках партиции/кумулятивные функции
4. Функции по плавающим окнам (time-series данные)


Misc:
spark.read.json - позволяет читать не только файлы, но и Dataset[String], содержащие JSON строки
spark.read.csv - позволяет читать заархивированные csv-файлы

map/flatMap == переход к ds
=> десериализация (Internal row => Java object) => MapPartition => сериализация (Java object => Internal row)

В Spark 3 появилась возможность написания плагинов для кастомных операторов (примеры - Direct Cassandra Join, Spark GPU)



__5

55-25 => 59-45
1-39-40 => 1-44-35

2-16-00

Планы выполнения задач:
Любой job в Spark SQL имеет под собой план выполнения, который генерируется на основе написанного запроса. План запроса содержит операторы, которые потом превращаются в java код. Посколько одну и ту же задачу в Spark SQL можно выполнить по-разному, полезно смотреть в планы выполнения, чтобы:
1. Убрать лишние shuffle
2. Убедиться, что тот или иной оператор будет выполнен на уровне источника (predicate pushdown), а не внутри Spark
3. Понять, как будет выполнен join

Планы выполнения доступны в 2-х видах:
1. метод explain() у DF
2. на вкладке SQL в Spark UI

Физический план == набор физических операторов
show => физический оператор CollectLimit

Варианты реализации операторов:
1. С кодогенерацией - представлены конкретным java кодом, который компилируется janino
2. Без кодогенерации - представлены кодом на scala (например - toJson)

Логические планы:
1. Parsed - запрос, распарсенный каталистом в некоторое дерево
2. Analyzed - производится некоторая валидация (например - колонки, указанные в select есть в датафрейме/проверка типов и т.д.)
3. Optimized - оптимизированное дерево (например - перестановка физических операторов)
Из Optimized Logiсal Plan получается Physical Plan

Любой датафрейм под капотом представляет собой RDD[InternalRow]
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.rdd.RDD

InternalRow могут быть представлены:
1. GenericInternalRow - человекочитаемые данные
2. UnsafeRow - массив байт с некоторой метаинформацией

1. Spark составляет физический план выполнения запроса на основании написанного кода
2. При изучении плана запроса, можно понять, какие операторы будут применены в ходе обработке данных
3. План выполнения запросов - один из основных инструментов оптимизации запроса


Оптимизация соединений и группировок:

При выполнении join двух DF важно следовать рекомендациям:
1. Фильтровать данные до join'а
2. Использовать equ join
3. Если можно путем увеличения количества данных применить equ join вместо non-equ join'а - то делать именно так
4. Всеми силами избегать cross-join
5. Если правый DF помещается в памяти worker'а использовать broadcast

Виды соединений:
1. BroadcastHashJoin - самый быстрый:
  1.1 equ join
  1.2 using broadcast (в HIVE называется map side join).
2. SortMergeJoin:
  2.1 equ join
  2.2 sortable keys
3. BroadcastNestedLoopJoin:
  3.1 non-equ join
  3.2 using broadcast
4. CartesianProduct (cross):
  4.1 non-equ join
5. HashJoin - был отключен по умолчанию где-то в Spark 2.2 Можно включить, но SortMergeJoin - эффективнее

Выбор алгоритма основывается на условии соединения и размере датасетов


BroadcastHash Join:
1. Работает, когда условие - равенство одного или нескольких ключей
2. Работает, когда один из датасетов небольшой и полностью вмещается в память воркера
3. Оставляет левый датасет как есть
4. Копирует правый датасет на каждый воркер (должен влезать в хип воркера)
5. Составляет HashMap из правого датасета, где ключ - кортеж из колонок в условии соедиения
6. Итерируется по левому датасету внутри каждой партиции и проверяет наличие ключей в HashMap
7. Может быть использован атоматически, либо явно через broadcast(df)

BroadcastTimeoutException (по умолчанию 300с) => на воркере не хватает памяти для броадкаста датафрейма

Указание объема памяти на воркерах:
1. spark-submit: --executor-memory 4g
2. spark-submit: --conf spark.executor.memory=4g
3. spark-defaults.conf spark.executor.memory 4g


SortMerge Join - дефолтный вид джоина больших датафреймов
Падает если есть перекошенные партиции, не влезающие на воркер:
1. Работает, когда ключи соединения в обоих датасетах являются сортируемыми
2. Репартиционирует оба датасета в 200 партиций по ключам соединения
3. Сортирует партиции каждого из датасетов по ключам соединения
4. Используетя сравнение левого и правого ключей, обходит каждую пару партиций и соединяет строки с одинаковыми ключами


BroadcastNestedLoop Join:
1. Работает, когда один из датасетов небольшой и полностью вмещается в память воркера
2. Оставляет левый датасет как есть
3. Копирует правый датасет на каждый воркер
4. Проходится циклом по каждой строке партиции левого датасета + вложенный цикл для обхода правого датасета и проверяет условие соединения
5. Может быть использован автоматически, либо явно через broadcast(df)


CartesianProduct - стоит избегать:
1. Создает пары из каждой партиции левого датасета с каждой партицией правого датасета, релоцирует каждую пару на один воркер и проверяет условие соединения
2. На выходе создает N*M партиций
3. Работает медленнее остальных и часто приводит к OOM воркеров

Признак CartesianProduct в UI - огромное количество партиций.


Снижение объема shuffle:
В ряде случаев можно уйти от лишних shuffle операций при выполнении соединения
Для этого оба DF должны иметь одинаковое партицирование - одинаковое количество партиций и ключ партицирования, совпадающий с ключом соединения

Разница между планами выполнения будет хорошо видна в Spark UI на графе выполнения в Jobs и плане выполнения в SQL

Optimal number of spark.sql.shuffle.partitions = num_workers * cores_per_worker * 3(или 4)

!!! После репартиционирования не имеет смысла делать cache т.к. данные уже будут лежать на дисках воркеров (т.к. результат репартиционирования - файлы)


Управление схемой данных:

В DF API каждая колонка имеет свой тип:
1. скаляр - StringType, IntegerType, ...
2. массив - ArrayType(T)
3. словарь - MapType(K, V)
4. структура - StructType

DF целиком имеет схему - описанную с помощью класса StructType

StructField имеет 3 атрибута - name/dataType/nullable

Иерархия типов:
1. DataType - абстрактный класс
  2. StructType
  3. ArrayType
  4. MapType
  5. AtomicType - абстрактный класс (protected[sql], скалярные типы):
  StringType/NumericType/TimeStampType/DateType
    6. NumericType - абстрактный класс:
    IntegralType/FractionalType
      7. IntegralType (private[sql]):
      IntegerType/LongType
      8. FractionalType (private[sql]):
      DoubleType/FloatType


Схема может быть использована:
1. При чтении источника - значительно ускоряет чтение (в худшем случае с автовыводом схемы - потребуется прочитать весь датасет)
2. При работе с JSON

1. Spark использует схемы для описания типов колонок, схемы всего DF, чтения источников и для работы с JSON.
2. Схема представляет собой инстанс класса StructType.
3. Колонки в Spark могут иметь любой тип. При этом вложенность словарей, массивов и структур не ограничена.


Оптимизатор запросов Catalyst:

Catalyst выполняет оптимизацию запросов с целью ускорения их выполнения и применяет следующие методы:
1. Column projection - позволяет избегать вычитывания ненужных колонок при работе с источниками (для поддерживающих это форматов - parquet/orc, для строковых форматов (например - json) - это будет указано в плане выполнения, но работать не будет).
+ Позволяет избегать вычисления ненужных колонок (например - groubBy).
2. Partition pruning - позволяет избежать чтения ненужных партиций. Работает только с файловыми форматами (json/orc/parquet/csv/text).
Best practice партиционирования - год/месяц/день.
3. Predicate pushdown - позволяет "протолкнуть" условия фильтрации данных на уровень datasource. 
В V1 Datasource API применяется дважды - на источнике - PushedFilters и в спарке - Filter.
== Physical Plan ==
    *(1) Filter (isnotnull(iso_country#86) AND (iso_country#86 = RU))
    +- *(1) ColumnarToRow
       // PushedFilters: [IsNotNull(iso_country), EqualTo(iso_country,RU)]
       +- FileScan parquet [ident#81,type#82,name#83,elevation_ft#84,continent#85,iso_country#86,iso_region#87,municipality#88,gps_code#89,iata_code#90,local_code#91,coordinates#92] Batched: true, DataFilters: [isnotnull(iso_country#86), (iso_country#86 = RU)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/mike/_learn/repos/newprolab/spark_1/lectures/src/main/resou..., PartitionFilters: [], PushedFilters: [IsNotNull(iso_country), EqualTo(iso_country,RU)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...
4. Simplify casts - убирает ненужные cast. Например при попытке каста к тому же самому типу данных.
5. Constant folding - сокращает количество констант, используемых в физическом плане.
6. Combine filters - объединяет фильтры.

Dynamic allocation:
1) Spark2 - воркеры начинают подниматься только когда появляются ожидающие обработки (pending) таски. До этого момента - воркеров нет. Можно настроить минимальное количество воркеров, которые всегда будут подняты.
2) Spark3 - сначала rdd/df запрашивает у ярна ресурсы => ярн выдает ресурсы =>
датафрейм начинает обсчитыватсья.




__6

58-45 => 1-04-50
1-57-05 => 2-05-05

Обзор источников данных:

Spark - это платформа для обработки распределенных данных. Она не отвечает за хранение данных и не завязана на какую-либо БД или формат хранения, что позволяет разработать коннектор для работы с любым источником. Часть распространенных источников доступна "из коробки", часть - в виде сторонних библиотек.

На текущий момент Spark DF API позволяет работать (читать/писать) с большим набором источников:
1. Текстовые файлы:
  1.1 Json.
  1.2 Text.
  1.3 Csv.
2. Бинарные файлы:
  2.1 Orc.
  2.2 Parquet.
  2.3 Delta.
3. Базы данных:
  3.1 Elastic.
  3.2 Cassandra.
  3.3 Jdbc.
  3.4 Redis.
  3.5 Mongo.
4. Стриминг системы:
  4.1 Kafka.

Для текстовых файлов поддерживаются различные кодеки сжатия (например - lzo/snappy/gzip). Поддерживается чтение из локальной файловой системы/HDFS/S3.

Добавление поддержки:

Чтобы добавить поддержку источника в проект, необходимо:
1.1 Найти нужный пакет на https://mvnrepository.com
1.2 Выбрать актуальную версию для Scala 2.12.
1.3 Скачать jar или скопировать команду для нужной системы сборки.

  "org.elasticsearch" %% "elasticsearch-spark-20" % "7.16.3"
  
2. Добавить зависимость в libraryDependencies в файле build.sbt:
libraryDependencies += "org.elasticsearch" %% "elasticsearch-spark-20" % "7.16.3"

3. Добавить зависимость в приложение одним из способов:
3.1 Добавить зависимость в spark-submit:
spark-submit --packages org.elasticsearch:elasticsearch-spark-20_2.12:7.16.3
Автоматическая загрузка джарников в кэш хоста, на котором запускается submit.
При запуске распределенного приложения джарники будут перемещены в рабочую
директорию драйвера/воркеров и добавлены в classpath.
  
3.2 Добавить jar файл в spark-submit:
spark submit --jars /path/to/elasticsearch-spark-20_2.12-7.16.3.jar
драйвер + воркеры
  
3.3 Добавить зависимость в spark-defaults.conf:
spark.jars.packages org.elasticsearch:elasticsearch-spark-20_2.12:7.16.3
  
3.4 Добавить jar файл в spark-defaults.conf:
spark.jars /path/to/elasticsearch=spark-20_2.12-7.16.3.jar
  
3.5 В рантайме с помощью кода:
spark.sparkContext.addJar
!!! Добавляет только на воркеры.


Использование в коде:

Конфиги источника задаются одним из способов:
1. Через spark-submit:
spark-submit --conf spark.es.nodes=localhost:9200
  
2. В spark-defaults.conf:
spark.es.nodes localhost:9200
  
3. В коде через SparkSesson:
spark.conf.set("spark.es.nodes", "localhost:9200")
  
4. В коде при чтении:
val df = spark.read.format("elastic").option("es.nodes", "localhost:9200")...
val df = spark.read.format("elastic).options(Map("es.nodes" -> "localhost:9200"))...

5. В коде при записи:
df.write.format("elastic").option("es.nodes", "localhost:9200)...
df.write.format("elastic").options(Map("es.nodes" -> "localhost:9200"))...
  
1. Spark позволяет работать с большим количеством источников.
2. Поддержка источника всегда добавляется на уровне JVM (даже для pyspark) путем добавления в java classpath нужного класса.
3. Добавить поддержку источника можно по-разному, однако в большинстве случаев следует избегать "хардкода".
4. Можно разработать свой коннектор на Datasource API (v1/v2).


Текстовые форматы:

Spark позволяет хранить данные в текстовом виде в форматах text/json/csv.
1. Json - JSON строки (не массив JSON документов, а именно раздельные строки, разделенные \n).
2. Csv - плоские данные с разделителем.
3. Text - просто текстовые строки, вычитываются как DF с единственной колонкой  value: String.

Преимущества:
1. Простота интеграции.
2. Поддержка партицировани и сжатия.

Недостатки:
1. Отсутствие оптимизаций.
2. Низкая скорость чтения сжатых данных.
3. Слабая типизация.

Запись данных происходит в директорию, внутри которой будут файлы с данными. Это свойство является общим для всех файловых форматов в Spark.
Каждый воркер записывает свою партицию на файловую систему (поэтому указывается именно директория, а не имя файла).

SUCСESS/crc файлы вредны т.к. знимают память немноды.
1_000_000 файлов в HDFS ~ 1 Gb хипа в нэйм ноде.
Best practiсe - в HDFS маленькие файлы следует объединять в большие + удалять ненужные файлы (SUCСESS/crc).

!!! При сжатии данные будут занимать меньше места, но у этого решения есть существенный минус - при чтении каждый сжатый файл текстового формата превращается ровно в 1 партицию в DF.
При работе с большими датасетами это означает:
1. Если файлов мало и они большие, то воркерам может не хватить памяти для их чтения (т.к. один сжатый текстовый файл нельзя разбить на несколько партиций).
2. Если текстовых файлов много и они маленькие - получаем увеличенный расход памяти в heap HDFS NameNode.

Партицированное хранение данных позволяет использовать partition pruning и быстро фильтровать данные по колонкам партицирования.
Колонок по которым выполнено партицирование - не будет в составе сохраненного датафрейма (т.к. в противном случае было бы дублирование).

В момент старта спарк сессии определяется с какой файловой системой будет работать Spark - если выставлена переменная окружения HADOOP_CONF_DIR - спарк будет работать с HDFS.

Режимы зыписи:
Spark позволяет выбирать режим записи с помощью метода mode:
1. !!! Overwrite - перезаписывает всю директорию целиком. При работе с базой часто (зависит от коннектора) превращается в Truncate.
2. Append - дописывает новые файлы к текущим.
3. Ignore - не выполняет запись (no operation режим).
4. Error/ErrorIfExists - возвращает ошибку, если директория уже существует.

Семплирование:
Форматы csv/json позволяет автоматически выводить схему из данных. При этом по-умолчанию Spark прочитает все данные и составит подходящую схему. Однако при работе с большим датасетом, это занимает продолжительное время.
Решение - опция samplingRatio.

1. При чтении/записи поддерживаются кодеки сжатия, но это создает дополнительные накладные расходы (cpu) и небезопасно на больших объемах датасетов (OOM).
2. При записи в текстовые форматы Spark не выполняет валидацию схемы и формата.
3. При включенном выведении схемы из источника - чтение из текстовых форматов происходит дольше.
4. Не использовать текстовые форматы в проде. В случае необходимости хранения json - 
сохранять json в parquet.


ORC/Parquet:

В отличие от обычных текстовых форматов, ORC/Parquet изначально спроектированы под распределенные системы хранения и обработки.
Они являются:
1. Колоночными - в них есть колонки и схема, как в таблицах БД.
2. Бинарными - прочитать обычным текстовым редактором их не получится.

ORC/Parquet имеют похожие показатели производительности и архитектуру, но Parquet используется чаще.

Преимущества:
1. Наличие схемы данных в файле.
2. Блочная компрессия.
3. Для каждого блока для каждой колонки вычисляется min/max, что позволяет ускорить чтение.

Недостатки:
1. Нельзя дописывать/менять данные в существующих файлах.
2. Необходимо делать compaction.

По аналогии с текстовыми форматами, при записи Spark создает директорию и пишет туда все непустые партиции.

Последовательность в формате записи - snappy.parquet - вместо например gson.gz.
При использовании компрессии сам parquet файл не помещается в сжатый контейнер - вместо этого компрессии подлежат блоки с данными (row group).
Это полностью снимает ограничение, из-за которого чтение сжатых текстовых файлов происходит в 1 поток в 1 партицию.

Schema evolution:
При работе с ORC/Parquet, часто воникает вопрос эволюции схемы - изменения структуры данных относительно первоначальных.

Best practice - создавать новую колонку с измененным типом данных (а не изменять тип существующей).

Parquet Tools (для ORC - есть ORC Dump) - для диагностики и решения проблем, связанных с parquet.
Работает с локальной ФС/hdfs.
Она позволяет:
1. Получить схему файла.
2. Вывести содержимое файла в консоль.
3. Объединить несколько файлов в один.

Если данные пришли в json, то лучше хранить их в parquet. Размер будет больше (~ +20%),
но получаем возможность сжатия без вреда для многопоточости.

1. Форматы orc/parquet позволяют эффективно работать со структурированными данными.
2. Производительность orc/parquet на порядок выше обычных текстовых файлов.
3. Данные форматы поддержживают сжатие на блочном уровне, что позволяет избегать проблем с многопоточным чтением.
4. Форматы поддерживают добавление новых колонок в схему, но не изменение текущих.


Elastic - документировання распределенная база данных, ориентированная на сложный
полнотекстовый поиск (построен на индексном движке Lucene).

Преимущества:
1. Удобный графический интерфейс Kibana.
2. Полнотекстовый поиск по любым колонкам.
3. Встроенная поддержка timeseries.
4. Поддержка вложенных структур.
5. Возможность записи данных с произвольной схемой (как плюс так и минус).
6. Возможность перезаписывать данные по ключу документа (=> отсутствие дубликатов - exactly once consistency).

Недостатки:
1. Ассиметричная архитектура:
  1.1 Клиентские ноды - отвечают за взаимодействие с пользовательскими запросами.
  1.2 Дата ноды - отвечают за хранение данных.
  1.3 Мастер ноды - обеспечивают работу кластера.
2. Скорость записи ограничена самым медленным узлом.
3. Большие накладные расходы CPU на индексирование.
4. Ротация (распределение данных по хостам) шардов не всегда проходит хорошо (перекосы).

В Elastic есть несколько основных сущностей:

1. Index - представляет собой "таблицу с данными", если проводить аналогию с реляционными БД. Данные в elastic обычно хранятся в виде индексов, разбитых на сутки (foo-2020-05-29, foo-2020-05-30 и т.д.). У каждого документа в индексе есть ключ _id (обеспечивает уникальность) и может быть метка времени, по которой в Kibana строятся визуализации.

2. Template - шаблон с параметрами, с которыми создается новый индекс (~схема для записи).
Задает ограничения (на уровне схемы документа) для записываемых документов (по умолчанию - ограничений нет).

0.0.0.0:5601 => Management => Dev Tools

Пример шаблона:
PUT _template/airports?include_type_name=true
{
  "index_patterns": ["airports-*"],
  "settings": {
    "number_of_shards": 1
  },
  "mappings": {
    "_doc": {
      "dynamic": true,
      "_source": {
        "enabled": true
      },
      "properties": {
        "ts": {
          "type": "date",
          "format": "strict_date_optional_time||epoch_millis"
        }
      }
    }
  }
}

?include_type_name=true - в 7.x без этого не будет работать, в 8.x будет удалено.

"index_patterns": ["airports-*"] - для всех индексов, начинающихся с airports-
"number_of_shards: 1 - все данные буду на одном шарде.
"dynamic": true - допустима запись документов с любой схемой, но в котором должно быть поле ts с типом "type": "date" формата "format": "strict_date_optional_time||epoch_millis"

3. Shard - индексы в elastic делятся на шарды. Это позволяет хранить индекс на нескольких узлах кластера.

4. Index Pattern - шаблон, применяемый к индексу на уровне Kibana. Позволяет настраивать форматирование и подсветку полей.

Перед тем, как начать писать в elastic с помощью Spark, нам необходимо создать шаблон, иначе индекс будет создан с параметрами по умолчанию и построить красивый pie chart в Kibana не получится. Это можно сделать с помощью Dev Tools в Kibana.

Management => Stack Management => Kibana => Index Patterns (опции для красивого отображения)
=> Create index pattern => Name == airports-* | Timestamp fiels == ts
=> Analytics => Discover (аналитика).

Основной пайплайн работы с elastic (и другими БД) - записывать в него данные (а не
читать - т.к. медленно).
Spark => запись данных в Elastic => аналитика/визуализация в Kibana.

1. Elastic - удобное распределенное хранилище документов, не накладывающее строгих ограничений на схему документов.
2. Elastic позволяет делать сложные запросы, включая полнотекстовые.
3. При работе с elastic, Spark часто использует filter pushdown.
4. Spark отлично подходит для того, чтобы писать в elastic. Однако чтение работает не очень быстро.
5. User friendly для аналитиков.
6. При отказе ноды, кластер будет временно недоступен.
7. Кластер только внутри одного дата-центра (и хорошо, если узлы будут расположены рядом на высокоскоростных свитчах).


Cassandra - распределенная табличная база данных.

Преимущества:
1. Высокая доступность данных (при отказе ноды -  кластер остается доступным).
2. Возможность построения гео-кластеров.
3. Высокая скорость записи и чтения.
4. Скорость ограничена самым быстрым узлом.
5. Линейная масштабируемость (без downtime).
6. Возможность хранить БОЛЬШИЕ объемы данных.
7. Возможность быстро получать строку по ключу на любом объеме данных.

Недостатки:
1. Слабая согласованность (eventual - т.е. есть возможность считать старые данные).
2. Бедный SQL (в кассандре называется Cassandra Query Language - CQL).
3. Отсутствие транзакций (есть lightweight transaction).

Cassandra имеет симметричную архитектуру. Каждый узел отвечает за хранение данных, обработку запросов и состояние кластера.

Расположение данных на нодах определяется значением хэш-функции (Murmur3) от partition key.
Высокая доступность (HA) обеспечивается за счет репликации (запись производится на n-1 следующих узлов в кольце кластера, n - фактор репликации).

Ключ может быть составным и состоять из:
1. Partition keys - часть ключа (колонки) по которым рассчитывается Murmur3 hash.
2. Clustering keys - определяют сортировку внутри каждой партиции.
Partition keys + Clustering keys = Composite key

От выбранных partition/clustering keys (определяются на этапе создания таблицы)
зависит возможность постороения различных типов запросов.

Кластер кассандры называется кольцом.
token(n1) = t1		range(t1, t2] => {n2, n3, n4}
token(n2) = t2		range(t2, t3] => {n3, n4, n5}
token(n3) = t3		range(t3, t4] => {n4, n5, n6}
...

Есть вторичные индексы по обычным колонкам (не являющимися ключами).

??? (точно ли) При записи производится Upsert == Update (для существующих ключей) + Insert.

В Cassandra есть:
1. Keyspace - аналог database - логическое объединение таблиц. На уровне keyspace устанавливается фактор репликации.
2. Table - таблицы, как в обычной БД.

Скорость чтения ОЧЕНЬ СИЛЬНО зависит от структуры таблицы и запроса. Если сделать запрос по колонке, являющейся partition key, то будет применена оптимизация filter pushdown (в Spark3 - Cassandra Filters: [["ident" = ?, 22MV]]) и запрос отработает очень быстро.

Если сделать запрос по колонке не являющейся partition key, то filter pushdown не отработает - БД будет вычитывать данные на всем кластере(full table scan) (в Spark3 - Cassandra Filters: []).

Скорость записи из Spark в Cassandra/Kafka - почти никогда не является узким местом.

1. Cassandra - одна из немногих БД, которая способна эффективно хранить большие объемы данных.
2. В Cassandra структура таблицы формируется, исходя из запросов, которые будут выполняться, а не наоборот.
3. В данной БД даннные обычно хранятся в денормализованном виде (если утрировать - то по таблице на каждый запрос).
4. Spark отлично подходит для записи данных в Cassandra, для чтения нужна настройка под конкретный профиль нагрузки.


PostgreSQL - классическая РБД:

При записи производится Insert.

По умолчанию чтение производится в 1 партицию.
Скорректировать это можно, используя параметры partitionColumn/lowerBound/upperBound/numPartitions.

Spark формирует схему для jdbc через запрос:
SELECT * FROM tableName WHERE 1 = 0 LIMIT 1;
Но гарантий оптимизации этого запроса в конкретной базе - нет, может быть FULL TABLE SCAN.

1. Spark позволяет работать с PostgeSQL через JDBC коннектор.
2. При использовании jdbc настройка партицирования задается вручную.

Компоненты кастомного коннектора:
1. RDD[InternalRow] - чтение данных в партицию.
2. schema: StructType.



__7

9-50
1-10-10 => 1-14-35
1-59-00 => 2-05-10
2-53-15

Общие сведения:

Системы поточной обработки данных:
1. Работают с непрерывным потоком данных.
2. Нужно хранить состояние стрима.
3. Результат обработки быстро появляется в целевой системе.
4. Должны проектироваться с учетом требований к высокой доступности.
5. Важная скорость обработки данных и время задержки (лаг).

Примеры систем поточной обработки данных:
1. Процессинг платежей по картам:
  1.1 Нельзя терять платежи.
  1.2 Нельзя дублировать платежи.
  1.3 Прстой сервиса недопустим.
  1.4 Максимальное время задержки ~1с.
  1.5 Небольшой поток событий.
  1.6 OLTP.
  
2. Обработка логов безопасности:
  2.1 Потеря единичных событий допустима.
  2.2 Дублирование единичных событий допустимо.
  2.3 Простой сервиса допустим.
  2.4 Максимальное время задержки ~1ч.
  2.5 Большой поток событий.
  2.6 OLAP.
  
Виды стриминг систем:
1. Real-time streaming (Flink):
  1.1 Низкие задержки на обработку.
  1.2 Низкая пропускная способность.
  1.3 Подходят для критичных систем (высокая доступность).
  1.4 Пособытийная обработка.
  1.5 OLTP.
  1.6 Exactly once consistency (нет потери данных и нет дубликатов).
  
2. Micro batch streaming (Spark):
  2.1 Высокие задержки.
  2.2 Высокая пропускная способность.
  2.3 Не подходит для критичных систем.
  2.4 Обработка батчами.
  2.5 OLAP.
  2.6 At least once consistency (во время сбоев могут возникать дубликаты).

Real-time streaming на Spark строить нельзя т.к. он не удовлетворяет требованиям.
  
1. Spark Structured Streaming является micro-batch системой.
2. При работе с большими данными обычно пропуская способность важнее, чем время задержки.


Rate streaming:

Самый простой способ создать стрим - использовать rate источник (простой счетчик). Созданный DF является streaming, о чем нам говорит метод создания readStream и isStreaming.
Rate хорошо подходит для тестирования приложений, когда нет возможности подключиться к потоку реальных данных.

Cинк == сток/потребитель/получатель == DataStreamWriter

В отличие от обычных датафреймов, у стриминговых нет таких методов как show/collect/take, также недоступен Dataset API. Поэтому для просмотра их содержимого, нужно использовать console синк и создать StreamingQuery. Процессинг начинается только после вызова метода start.
Trigger позоляет настроить частоту считывания/обработки новых данных.

Стриминговый датафрейм как и обычный имеет партиции (можно выполнять repartition/groupBy/join).
В заданный момент времени обрабатывается конкретный микробатч (микробатч под капотом == RDD[InternalRow]), состоящий из определенного числа партиций.

Автоматический перезапуск стрима:
1. Стрим упал внтури приложения, но приложение не упало - например ошибка трансформации/OOM на воркере.

Перезапуск возможен из приложения после анализа ошибки. 
Sink.start() является неблокирующей операцией - стрим работает асинхронно.

Try {
  val sq = sink.start()
  sq.awaitTermination(15000)
}

awaitTermination(15000) блокирует основной поток на 15с - далее стрим или продолжит работу или упадет с ошибкой.
Т.е. через каждые 15 секунд работы мы можем запустить свою логику - например rest api для управления стримом снаружи (стартовать/перезапускать).
Более простое решение - маркер файл на hdfs - если появляется (например stop.file) - остановить стрим.

2. Стрим упал и приложение упало - например зафейлились таски => зафейлился стрейдж => несколько перезапусков стейджа (если все еще фейлится) => фейл приложения.

Можно перезапустить через oozie/airflow/cron.
Если платформа стримовая - стрим надо мониторить.
!!! Стрим должен работать в единственном экземпляре - zookeeper/consul/маркер файл на hdfs.

Socket Source - лучше не использовать.

ForeachBatch Sink - позволяет применить произвольную функцию к микробатчу.

Чекпоинты -  метаинформация, позволяющая корректно перезапускать стримы с места остановки.

!!! Spark не читает файлы:
1. tmp
2. начинающиеся с "_"
3. начинающиеся с "."

Best practice стриминговой записи на hdfs:
1. Repartition - уменьшить количество партиций
2. Должен быть compaction - задача в фоне, объединяющая мелкие файлы.

Варианты записи стриминга более крупными частями:
1. trigger(Trigger.ProcessingTime("10 seconds")) - увеличить время триггера.
2. createParquetSink(streamDf.repartition(1), "s1.parquet") - репартицирование в меньшее количество партиций.

Производительность стрима завязана на количество партиций и количество воркеров (как и для батчевых задач). Часто ограничена пропускной способностью приемника (elastic/db)

Оценка производительности стрима:
1. lastProgress/recentProgress:
  1.1 inputRowsPerSecond - прочитано строк/c.
  1.2 processedRowsPerSecond - обработано строк/c - более честный параметр.

Параллельно внутри одного Spark приложения можно запускать несколько стримов (т.к. они работают асинхронно) - антипаттерн.

1. rate - самый простой способ создать стрим для тестирования приложений.
2. Стрим начинает работу после вызова метода start() и не блокирует основной поток программы.


File Streaming:

Spark позволяет запустить стрим, который будет "слушать" директорию (локальная фс/hdfs) и читать из нее новые файлы. При этом за раз будет прочитано количество файлов, установленное в параметре maxFilesPerTrigger.
В этом кроется одна из основных проблем данного источника. Поскольку стрим, сконфигурированный под чтение небольших файлов, может "упасть", если в директорию начнут попадать файлы большого объема. 

В чекпоинте хранится информация об обработанных файлах.

Можно стримить как плоскую так и вложенную (/*) структуру директорий.
Можно стримить из parquet/orc/json/text/csv - важно, чтобы был единый формат данных для всего стрима.

Недостатки File streaming:
1. Входной поток можно ограничить только максимальным количеством файлов, попадающих в батч.
2. Гранулярность обработки - батч. Если стрим упадет в середине обработки батча, то при перезапуске все файлы этого батча будут обрабатываться снова. 

!!! File streaming не стоит использовать на проде.



Kafka streaming:

Apache Kafka - самая распространенная в мире система, на основе которой строятся приложения для поточной обработки данных.

Предназначена для записи/чтения больших объемов данных.
Снижает связанность системы - приложение продюсер никак не связано с приложением консьюмером. Можно обновлять/останавливать продюсер/консьюмер приложения независимо.

Преимущества:
1. Высокая пропускная способность.
2. Высокая доступность за счет распределенной архитектуры (выход из строя ноды не приводит к остановке работы) и репликации.
3. У каждого сообщения есть свой номер, который называется offset, что позволяет гранулярно сохранять состояние стрима. Это позволяет обработать снова уже обработанные сообщения. Сообщение при чтении не удаляется.

Технически Kafka - не брокер очередей (нет понятия очереди), а distributed commit log (распределенный лог изменений)

Архитектура системы:

Topic - это таблицы в Kafka. Мы пишем данные в топик и читаем данные из топика. Топик как правило распределен по нескольким узлам кластера (партиции) для обеспечения высокой доступности и скорости работы с данными.

Partition - это блоки, из которых состоят топики. Партиция представляет собой неделимый блок, который хранится на одном из узлов. Топик может иметь произвольное количество партиций. Чем больше партиций - тем выше параллелизм при чтении/записи, однако большое число партиций в топике может привести к замедлению работы всей системы.

Replica - каждая партиция может иметь несколько реплик (т.е. репликация в Kafka работает на уровне партиций). Внешние приложения всегда работают (читают/пушат) с основной репликой. Остальные реплики являются дочерними и не используются во внешнем IO - только синхронизируют данные из основной реплики. Если узел, на котором расположена основная реплика, падает, то одна из дочерних реплик становится основной и работа с данными продолжается.

Message - это данные, которые мы пишем и читаем в Kafka - представлены кортежем (Key, Value).
Ключ используется не всегда (может иметь значение null). Сериализация/десериализация данных всегда происходят на уровне клиентов Kafka. Сама Kafka ничего о типах данных не знает и хранит ключи/значения в виде массива байт.

Ключ нужен для партиционирования данных внутри топика.

Offset(Long - 2^64) - это порядковый номер сообщения в партиции. При записи сообщения (сообщение всегда пишется в одну из партиций топика) Kafka помещает его в топик с номером n+1, где n - номер последнего сообщения в этом топике. Оффсеты растут независимо по каждой партиции.

Адрес сообщения в Kakfa - имя топика : id партиции : оффсет

Producer - это приложение, которое пишет в топик. Producer'ов может быть много. Параллельная запись достигается за счет того, то каждое новое сообщение попадает в случайную (round-robin partitioning) партицию топика (если не указан key).

Consumer - это приложение, читающее данные из топика. Consumer'ов может быть много, в этом случае они называются consumer group (группе назначается идентификатор - id). Параллельное чтение достигается за счет распределения партиций топика между consumer'ами в рамках одной группы. Каждый consumer читает данные из "своих" партиций и ничего про другие не знает. Если consumer падает - "его" партиции переходят другим consumer'ам (происходит перевыбор партиций - на это время чтение данных прекращается).

Если консьюмеров больше чем партиций - лишние консьюмеры будут использовать ресурсы впустую.

Сommit - называют сохранение информации о факте обработки сообщения с отдельным оффсетом.
Поскольку оффсеты для каждой партиции топика свои, то и информация о последнем обработанном оффсете хранится по каждой партиции отдельно. Обычные приложения пишут коммиты в специальный топик Kafka, который имеет название __consumer__offsets__.

Управление оффсетами в Spark реализованы через (каждому батчу в) чекпоинты на фс (например hdfs).

Retention - поскольку кластер Kafka не может хранить данные вечно, то в ее конфигурации задаются пороговые значения по объему и/или времени хранения для каждого топика, при превышении которого данные удаляются. Например - если у топика A установлен retention по времени 1 месяц, то данные будут хранится в системе не менее одного месяца (и затем будут удалены одной из внутренних подсистем).

Best practice - для удобного последующего поиска оффсета с нужным таймстэмпом - после обработки батча записывать в БД offset:timestamp.
Т.к. обходить топики и каждый раз искать нужные оффсеты с нужными таймстэмпами - долго.


Работа с Kafka с помощью Static DataFrame:

Spark позволяет работать с Kafka как с обычной базой данных. Для этого необходиммо подготовить датафрейм с 2-мя колонками:
1. value: String - данные, которые мы хотим записать.
2. topic: String - топик, куда писать каждую строку датафрейма.

Kafka использует Zookeeper для синхронизации узлов (ранее в зукипере также хранились оффсеты).

Чтение из Kafka имеет несколько особенностей:
1. По умолчанию читается все содержимое топика. Поскольку обычно в нем много данных, эта операция может создать большую нагрузку на кластер Kafka и Spark приложение.
2. Колонки value/key имеют тип binary, который необходимо десериализовать.

По умолчанию параметр startingOffsets имеет значение earliest, а endingOffsets - latest. Поэтому если не указывать эти параметры - Spark прочитает содержимое всего топика.

Чтобы прочитать только определенную часть топика, необходимо задать минимальный и максимальный оффсет для чтения с помощью параметров startingOffsets/endingOffsets.

Чтобы получить данные из топика - их необходимо десериализовать. Формат данных может быть произвольным (если json - достаточно cast("string")).

Consumer group управляется Spark автоматически, поэтому собственный механизм Kafka - не используется.

При использовании статических датафреймов с Kafka - если нужно хранить последний прочитанный оффсет - нужно делать руками.


Работа с Kafka с помощью Streaming DF:

При создании streamingDf необходимо помнить:
1. startingOffsets - по умолчанию имеет значение latest.
2. endingOffsets использовать нельзя (стрим бесконечен).
3. Количество сообщений за батч можно (и нужно) ограничить параметром maxOffsetsPerTrigger - по умолчанию он не задан, и первый батч будет содержать данные всего топика.

Гранулярность - сообщение топика Kafka.

Если maxOffsetsPerTrigger ~= lastProgress.processedRowsPerSecond (? или другой параметр) стрим лагает, т.е. вычитывет старые данные.

Чекпоинты лучше писать на hdfs (чекпоинты на локальной файловой системе могут быть повреждены при отключении питания).

В чекпоинте лежат последние оффсеты по каждому топику.
src/main/resources/l_7/chk/s3.kafka/offsets

Если нужно перечитать данные из Kafka - нужно удалить файл оффсета и в парамете startingOffsets указать нужное значение.

В Spark3 вроде бы есть gracefull остановка.




__8

4-45 => 6-10
44-30 => 50-40
1-44-00 => 1-50-00

Stateful streaming:

Stateful streaming - это вид поточной обработки данных, при которой при обработке батча с данными используются данные из предыдущих батчей.

Все операции с использованием select/filter/withColumn (кроме операций с плавающими окнами) являются stateless.

Stateless stream:
1. Стрим не выполняет операций, требующих работы с данными из разных батчей.
2. После обработки батча, стрим "забывает" про него.
3. Высокая пропускная способность.
4. Небольшое количество файлов и общий объем чекпоинта.
5. Возможность вносить существенные правки в код стрима без пересоздания чекпоинта.

Если при обработке стрима используются такие методы как join()*/groupBy()/dropDuplicates() или функции c плавающими окнами то:
1. В стриме должна быть колонка с временной меткой, но основе которой можно определить watermark.
2. Стрим будет работать медленнее, чем stateless.
3. В чекпоинте будет МНОГО файлов (хранение агрегатов).
4. При внесении изменений в код стрима с большой вероятностью придется пересоздавать чекпоинт. 

* Stream-Static left/inner/left anti join работают в append режиме без обязательного указания watermark.
??? Только для stream-stream join


Удаление дубликатов:

Spark позволяет удалять дубликаты данных в стриме. Это можно сделать 2-мя способами:

1. Без использования watermark:
  1.1 Хэш каждого элемента будет сохраняться в чекпоинте.
  1.2 В стриме полностью исключаются дубликаты.
  1.3 Если число уникальных значений большое - со временем начнется деградация стрима (из-за 1.1).
  
2. С использованием watermark:
  2.1 Хэш старых событий удаляется из чекпоинта.
  2.2 Появление дубликатов возможно, если они приходят с задержкой N > watermark.
  2.3 Стрим не деградирует со временем.
  2.4 Колонка, по которой делается watermark, должна быть включена в dropDuplicates.

Watermark - пороговое значение, рассчитывающееся на основании колонки с таймстемпом данных.
!!! Т.е. в стриме обязательно наличие колонки с временной меткой.

Алгоритм работы watermark - есть несколько переменных:
1. Maximum Observable Timestamp (MOT) - обновляется с каждым батчем (максимальный таймстэмп в предыдущих батчах).
2. Порог - Threshold - например 5 минут.
3. Время события - eventTimestamp.

!!! Если eventTimestamp < MOT - threshold - событие игнорируется.

dropDuplicates - делает неявное репартицирование.

При использовании watermark фильтрация дублей происходит в 2 этапа:
1. Отбрасываются старые события не подходящие под eventTimestamp < MOT - threshold.
2. Удаляются дубликаты по полям watermark + указанные поля.

Нахождение оптимальной watermark - баланс между условиями бизнеса и нагрузкой на стрим (чем больше watermak - тем больше хэшей хранить).

Если необходимы абсолютно уникальные значения (а не только в рамках watermark) - нужно проверять совпадение полей не через statefull streaming, а через udf которая будет обращаться к очень быстрой БД (например redis) в каждом батче.


Агрегаты:

!!! При построении агрегатов на стриме важно задать правильный ouputMode, который может иметь 3 значения:

1. Append - по умолчанию. После обработки батча Spark "забывает" про него.
В append режиме в синк будут записаны только завершенные окна с данными в момент window_right_bound + watermark_value. Удобно когда нужно ЕДИНОЖДЫ записать ФИНАЛЬНЫЙ результат (больше не изменится).
!!! Без watermark - не работает.

2. Update - будем получать только дельты, т.е. строки, которые поменялись в агрегате после обработки текущего батча.
!!! Записывать в таком режиме в cassandra/mongo - дешевле чем в complete, т.к. перезаписываться будут только изменения.

Можно использовать watermark:
  2.1 watermark + указанные поля.
  2.2 агрегаты на плавающих окнах.

3. Complete - каждый батч будет дописываться в бесконечную таблицу.
Агрегаты будут рассчитываться на каждом батче по ВСЕЙ таблице. Если кардинальность ключей высокая - со временем стрим будет деградировать.

!!! В complete нельзя использовать watermark => можно использовать для statefull, только при низкой кардинальности ключей (~ до 1_000_000 значений).

Если писать в паркет - на каждом батче будет записываться новый файл, если использовать cassandra/mongo - на каждом батче ВСЕ данные будут перезаписываться (большая нагрузка на БД).



Агрегаты на плавающих окнах:

Плавающее (sliding) окно позволяет сгруппировать события в окна определенного размера (по времени). При этом, поскольку каждое событие может находиться одновременно в нескольких окнах, то общий размер агрегата существенно увеличится.

Окно задается при создании агрегата с помощью функции window внутри groupBy.
В параметрах указывается длина окна и расстояние между точкой начала текущего окна и следующего окна.

1. Watermarks поддерживаются в режимах append/update.
2. Плавающее окно имеет два параметра - размер окна и сдвиг текущего окна относительно следующего.


Соединения:

Spark позволяет делать:
1. Stream - Static join (работают как джоины на обычных датаферймах):
  1.1 Inner.
  1.2 Left Outer.
  1.3 Left Anti.
2. Stream - Stream join:
  2.1 Inner.
  2.2 Left Outer.
  2.3 Right Outer.
  
Stream-Static join - может использоваться для:

1. Обогащения стрима фактами - left outer.
!!! Best practice - соединять с небольшой таблицей в формате parquet. Не стоит джойнить с большими таблицами из БД (даже из быстрых типа Cassandra) т.к. для джоина надо прочитать оба датафрейма целиком (механика джоина) - т.е. на каждом батче будет вычитываться вся огромная таблица из БД.
В Spark3 - появился оператор directCassandraJoin, который работает быстрее BroadcastHashJoin, но для highload также не подойдет.
Подобную логику можно реализовать с помощью udf с connection pool.

2. Фильтрация по whitelist - inner.

3. Фильтрация по blacklist - left anti.
  
!!! Можно использовать watermark для удаления старых данных.


Stream-Stream join:

Для соединения двух стримов необходимо одно из двух:

1. Добавить к условию соединения равенство двух окон - джоин событий по интервалу времени (таймстэмп одного и того же события - почти всегда будет отличаться).
 
2. Добавить к условию соединения сравнение двух временных меток.
Можно использовать округление таймстэмпов.

Стейт стрима хранит все данные по незакрытым окнам.

StreamingSymmetricHashJoin - технически работает как HashJoin (был удален из Spark) - строит карту хэш-мап для партиций.

Получение отброшенных (ватермаркой) событий:
В lastProgess => раздел eventTime - есть значения watermark + метрики времени - на основании этих данных можuо потенциально получать отброшенные события - создать еще 1 стрим и на основании batchId/eventTime получать отброшенные события (но получается двойное чтение источника + возможна рассинхронизация стримов). 
Нормального решения - нет (только постпроцессинг).




__9

45-30 => 54-00
2-02-55 => 2-09-00

Foreach Batch Sink:

Проблемы обычных синков:

!!! Два стрима, даже если они созданы на основе одного датафрейма, не могут использовать общий чекпоит =>
1. Стримы являются независимыми и работают асинхронно.
2. Чтение данных из источника происходит у каждого стрима НЕЗАВИСИМО и АСИНХРОННО (серьезное увеличение нагрузки на источник).
3. Один стрим может начать отставать (лагать) от другого стрима.

Обычные синки не позволяют использовать кастомную логику работы стрима (например в зависимости от поступающих данных).


В Spark_2.4 появился foreachBatch синк - это синк, позволяющий применить произвольную функцию к каждому батчу в стриме, работая с ним, как со статическим датафреймом.

1. Обычные синки позволяют создавать простые пайплайны, в реальной жизни чаще используются foreachBatch.

2. foreachBatch - каждый микробатч, передающийся в foreachBatch будет представлен как статический:
  2.1 Можно использовать любое API внутри функции.
  2.2 Можно использовать cache()/persist().
  2.3 Можно выполнять запись в несколько разных мест.
  2.4 Поддерживает режимы append/update/complete.
  2.5 Использовать batchId.
  

FAIR Scheduler:

До этого момента мы всегда выполняли action'ы над датафреймами один за другим, т.е. последовательно.

Данный подход имеет  существенный недостаток - низкая утилизация ресурсов, предоставленных Spark приложению, поскольку каждое действие блокирует основной поток на драйвере и не позволяет выполняться следующим, даже если у приложения еще есть свободные ресурсы.

Вышеписанная проблема не является критичной для обычных ETL приложений, однако в стримах, где задержка обработки данных является одним из ключевых параметров, простой выделенных ресурсов недопустим.  

Используя метод par, мы можем запустить вычисления на драйвере одновременно для всех датафреймов, однако планировщие Spark приложения работает в режиме FIFO (все доступные ядра забирает первая джоба, если ядер больше чем нужно первой джобе - они отдаются второй и т.д.), поэтому на воркерах партиции разных датарфеймов будут все еще обрабатываться ПОСЛЕДОВАТЕЛЬНО - плохо, например при записи в несколько источников (пиковая нагрузка на каждый источник)

Если переключить режим планировщика в FAIR, то все действия буду выполняться ПАРАЛЛЕЛЬНО.
FIFO - плохой вариант при записи в несколько мест (например в 3 базы) =>
FAIR - даст меньшую нагрузку на базы + лучше параллелизм.

Для этого необходимо устновить две опции в spark-defaults.conf:
1. spark.scheduler.mode FAIR
2. spark.scheduler.allocation.file /path/to/fairscheduler.xml.
Если запуск производится в cluster mode - нужно указать данный файл через --file.

Файл fairscheduler.xml должен содержать:
<?xml version="1.0"?>
<allocations>
  <pool name="default">
    <schedulingMode>FAIR</schedulingMode>
    <weight>1</weight>
  </pool>
</allocations>

1. Spark позволяет запускать действия параллельно.
2. Для параллельного запуска действий должны быть выполнены следующие условия:
  2.1 Планировщие Spark должен быть переведен в FAIR режим.
  2.2 Пул default должен быть переведен в FAIR режим.
  2.3 Действия должны запускаться на драйвере параллельно (можно использовать любое API, поддерживающее multithreading).

Для выполнения shuffle - драйвер должен знать где какие ключи лежат - драйвер составляет карту ключей на воркерах.

В одну и ту же папку нельзя одновременно записывать паркеты из разных датафреймов.

Future - позволяет более гранулярно управлять действиями, чем parallel collection.




__10

1-40
1-23-00 => 1-27-15

В BigDataTools можно работать с удаленным кластером Spark.

Тестирование можно производить на dev-кластере в режиме YARN (важно правильно параметризировать тесты). Вплоть до установки sbt на dev-кластер и запуска с нее.
Конфиги задавать через переменные окружения/отдельный yaml-файл.

InternalRow - строка с данными в датафрейме.

1. Стрим перед началом работы проверяет - есть ли новые данные в источнике =>
trait Source def getOffset: Option[Offset]
2. Если новые данные есть =>
trait Source def getBatch(start: Option[Offset], end: Offset): DataFrame
3. trait Source def commit - вызывается после каждого батча (getBatch).
Можно реализовать свою логику коммита - нужна базе а не спарку.
4. trait Source def stop - можно имплементировать кастомную логику действий после остановки стрима.

Из jdbc можно создать стриминговый источник.

!!! Минорная (2.13.x) версия скалы в джарнике ДОЛЖНА соответствовать версии скалы в дистрибутиве спарка.
./spark-shell =>
util.Properties.versionString - версия скалы.
sc.version - версия спарка.




__11

1-03-00 => 1-09-25
1-50-30

Настройка spark-submit окружения:

Дистрибутив Spark содержит в себе различные библиотеки, примеры конфигурационных файлов и набор утилит. Любое Spark приложение запускается одной из утилит:
1. spark-shell - запуск интерактивного Scala REPL с поддержкой Spark.
2. pyspark  - запуск интерактивного python шела с поддержкой Spark.
3. spark-submit - запуск Spark приложений, собранных в виде jar/py файлов с зависимостями.
spark-submit - обертка над специальным классом, запускающим приложение.

spark-submit/spark-shell/pyspark - находятся в каталоге bin.
При запуске Spark задач, под капотом используются именно эти утилиты. Достаточно, чтобы они были установлены только на те хосте, с которых происходит запуск приложения. На обычных узлах кластера они не нужны.

beeline - cmd утилита для подключения к thrift/hive для выполнение sql запросов.
spark-sql - позволяет выполнять нативные sql запросы (запускает спарк джобу).

./spark-submit --help =>
Usage: spark-submit [options] <app jar | python file | R file> [app arguments]
Usage: spark-submit --kill [submission ID] --master [spark://...]
Usage: spark-submit --status [submission ID] --master [spark://...]
Usage: spark-submit run-example [options] example-class [example args]

Options:
  --master MASTER_URL (выбор менеджера кластера):
  	1. spark://host:port - используется спарковский кластер менеджер.
  	Демоны на каждой ноде хоста. 1 общий application master на все спарк приложения.
  	Хороший вариант для бэкэнда (не нужно пускать пользователей, не нужна ad-hoc аналитика).
  	2. mesos://host:port
  	3. yarn - на каждое ярн приложение создается отдельный application master.
  	Приоритеты указываются в разрезе очередей.
  	Resource overcommitment - 
  	Preemption - механизм позволяющий, убивать контейнеры менее приоритетных задач, для
  	запуска контейнеров более приоритетных.
  	4. k8s://https://host:port
  	5. local
  	(Default: local[*]).
                              
  --deploy-mode DEPLOY_MODE
  	1.Сlient - драйвер будет запущен на локальной машине =>
  	все логи будут приходить в локальную консоль.
  	2. Cluster - драйвер будет запущен в кластере.
  	Приложение будет работать после отключения ssh-сессии.
  	Whether to launch the driver program locally ("client") or on one of
  	the worker machines inside the cluster ("cluster")
  	(Default: client).
 
  --class CLASS_NAME
  	Your application's main class (for Java / Scala apps).
  	
  --name NAME
  	A name of your application.
  	
  --jars JARS 
  	Comma-separated list of jars to include on the driver and executor classpaths.
  	
  --packages
  	Место хранения джарников - user/ivy2.
  	Comma-separated list of maven coordinates of jars to include on the driver and
  	executor classpaths. Will search the local maven repo, then maven central
  	and any additional remote repositories given by --repositories.
  	The format for the coordinates should be groupId:artifactId:version.
  	
  --exclude-packages
  	Comma-separated list of groupId:artifactId, to exclude while resolving the
  	dependencies provided in --packages to avoid dependency conflicts.
  	
  --repositories
  	Comma-separated list of additional remote repositories to search for the
  	maven coordinates given with --packages.
                              
  --py-files PY_FILES
  	Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps.
  	
  --files FILES
  	Comma-separated list of files to be placed in the working directory of each executor.
  	File paths of these files in executors can be accessed via SparkFiles.get(fileName).
  	
  --archives ARCHIVES
  	Comma-separated list of archives to be extracted into the
  	working directory of each executor.

  --conf, -c PROP=VALUE
  	Arbitrary Spark configuration property.
  	
  --properties-file FILE
  	Path to a file from which to load extra properties.
  	If not specified, this will look for conf/spark-defaults.conf.

  --driver-memory MEM
  	Memory for driver (e.g. 1000M, 2G) (Default: 1024M).
  	
  --driver-java-options
  	Например - выбор gc.
  	Extra Java options to pass to the driver.
  	
  --driver-library-path
  	Extra library path entries to pass to the driver.
  	
  --driver-class-path
  	Extra class path entries to pass to the driver.
  	Note that jars added with --jars are automatically included in the classpath.

  --executor-memory MEM
  	Memory per executor (e.g. 1000M, 2G) (Default: 1G).

  --proxy-user NAME
  	User to impersonate when submitting the application.
  	This argument does not work with --principal / --keytab.

  --help, -h 
  	Show this help message and exit.
  	
  --verbose, -v
  	Print additional debug output.
  	
  --version,
  	Print the version of current Spark.

 Cluster deploy mode only:
  --driver-cores NUM 
  	Number of cores used by the driver, only in cluster mode
  	(Default: 1).

 Spark standalone or Mesos with cluster deploy mode only:
  --supervise
  	If given, restarts the driver on failure.

 Spark standalone, Mesos or K8s with cluster deploy mode only:
  --kill SUBMISSION_ID
  	If given, kills the driver specified.
  	
  --status SUBMISSION_ID
  	If given, requests the status of the driver specified.

 Spark standalone, Mesos and Kubernetes only:
  --total-executor-cores NUM
  	Total cores for all executors.

 Spark standalone, YARN and Kubernetes only:
  --executor-cores NUM
  	Number of cores used by each executor.
  	(Default: 1 in YARN and K8S modes, or all available cores on the worker in standalone mode).

 Spark on YARN and Kubernetes only:
  --num-executors NUM
  	Number of executors to launch
  	(Default: 2).
  	If dynamic allocation is enabled, the initial number of executors will be at least NUM.
  	
  --principal PRINCIPAL
  	Для Кербероса.
  	Principal to be used to login to KDC.
  	
  --keytab KEYTAB
  	The full path to the file that contains the keytab for the principal specified above.

 Spark on YARN only:
  --queue QUEUE_NAME
  	The YARN queue to submit to (Default: "default").

В каталоге python/lib находятся зависимости для python - при настройке среды разработки - не нужно ставить pyspark в свой venv. Правильнее - добавить эти библиотеки из дистрибутива - избегаем возможных конфликтов версий в будущем.

В каталоге jars находятся все необходимые библиотеки, включая классы драйвера/воркеров, библиотеки для работы с Hadoop/встроенными источниками данных (например - parquet).

В каталоге conf находятся шаблоны конфигурационных файлов:
1. spark-defaults.conf - основной конфигурациооный файл. Общие для всех пользователей настройки нужно указать в spark-defaults.conf здесь, различные - через параметры спарк-сабмита/отдельные пропертис файлы (!!! полностью оверрайдит дефолтный).
2. log4j.properties - конфигурация логгеров - можно задать необходимый уровень логирования для различных компонентов Spark.
3. spark-en.sh - скрипт, в котором устанавливаются переменные окружения (например - HADOOP_CONF_DIR/YARN_CONF_DIR).
!!! Переменные окружения имеют больший вес, чем настройки в spark-defaults.conf.
4. metrics.properties
Можно отправлять метрики (java метрики, стримовые метрики) в Графит (например).
Встроенные метрики - не очень информативны.
!!! Библиотека dropWizard позволяет имплементить кастомные метрики.

Приоритет опций:
1. spark-submit
2. Переменные окружения
3. spark-defaults.conf

1. Наличие Hadoop не является необходимым условием для работы Spark.
2. spark-submit позволяет запустить Spark прложение как локально, так и на кластере.


Работа со Spark UI:

Каждое Spark приложение по умолчанию поднимает UI, который позволяет изучить состояние задачи и провести диагностику производительности. Данные, представленные в UI, также можно получить через REST API.

count запускает проекцию в 0 колонок (т.к. ему важно только количество строк) - поэтому это быстрая операция.

Количество воркеров - Spark UI => Executors.

Драйвер запустился => Yarn status == Running - далее начитаю подниматься контейнеры для воркеров.

Кэшировать паркет не имеет особого смысла т.к. это быстрый источник.
Медленные источники - базы данных/статический дф из кафки/заархивированные текстовые форматы.

Запись в паркет промежуточных результатов - страховка при ошибках в долгих джобах.




__12

19-50
55-30 => 1-02-30

Рекомендуется spark.locality.wait = 0s

InternalRow является упорядоченной.


.withColumn("domain", callUDF("parse_url", 'domain, lit("HOST")))
https://spark.apache.org/docs/3.2.1/api/sql/#parse_url

